{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DR_detection_VGG16",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcUi31vdIcWQ",
        "outputId": "3e75663f-cb1b-4a30-a7bd-9ae75c445e2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import basename, join, exists"
      ],
      "metadata": {
        "id": "pIkV16mCIqwg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder=r\"/content/drive/MyDrive/My_projects _and _datasets/Final_work/APTOS/Train/\"\n",
        "total=0\n",
        "print('---Training set details----')\n",
        "for sub_folder in os.listdir(folder):\n",
        "  no_of_images=len(os.listdir(folder + sub_folder))\n",
        "  total+=no_of_images\n",
        "  print(str(no_of_images) + \" \" + sub_folder + \" images\")\n",
        "\n",
        "print(\"Total no. of images \",total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wua2_EjPIrpO",
        "outputId": "30d72e9f-f6e7-40fb-ad74-8e299df4cc48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Training set details----\n",
            "1676 with_DR images\n",
            "1608 without_DR images\n",
            "Total no. of images  3284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder=r\"/content/drive/MyDrive/My_projects _and _datasets/Final_work/APTOS/Test/\"\n",
        "total=0\n",
        "print('---Test set details----')\n",
        "for sub_folder in os.listdir(folder):\n",
        "  no_of_images=len(os.listdir(folder + sub_folder))\n",
        "  total+=no_of_images\n",
        "  print(str(no_of_images) + \" \" + sub_folder + \" images\")\n",
        "\n",
        "print(\"Total no. of images\",total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RswZl2C3KUK0",
        "outputId": "9e3fc3ad-a785-4965-bbc9-e0a76e84706e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Test set details----\n",
            "181 with_DR images\n",
            "197 without_DR images\n",
            "Total no. of images 378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(777)\n",
        "import time\n",
        "import keras as keras\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation,Flatten\n",
        "from keras.layers import merge,Input\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.vgg16 import preprocess_input as pi_vgg16\n",
        "from keras.applications.inception_v3 import preprocess_input as pi_incep\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as pi_resnet\n",
        "from keras.applications.vgg19 import preprocess_input as pi_vgg19\n",
        "from keras.applications.xception import preprocess_input as pi_xcep \n",
        "from keras.models import load_model\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.metrics import accuracy_score\n",
        "from  numpy import mean \n",
        "from numpy import std\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
        "from keras.layers import GlobalAveragePooling2D, Concatenate\n",
        "from keras.layers import BatchNormalization,Dropout\n",
        "from keras.layers import Lambda\n",
        "from keras.regularizers import l2\n",
        "import math\n",
        "from keras import backend as K\n",
        "from keras.metrics import categorical_accuracy\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "xIUl-q7GJKpE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_height =224\n",
        "img_width = 224\n",
        "batch_size =32\n",
        "input_shape = (img_width, img_height, 3)"
      ],
      "metadata": {
        "id": "yp8K4KPqLNZ1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-----------------Image Augmentation for VGG16--------------\")\n",
        "\n",
        "random_seed = np.random.seed(1142)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    validation_split= 0.2,\n",
        "    zoom_range=0.2)\n",
        "    #shear_range=0.2)\n",
        "\n",
        "train_generator_vgg16 = train_datagen.flow_from_directory(\n",
        "    \"/content/drive/MyDrive/My_projects _and _datasets/Final_work/APTOS/Train/\",\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    seed = random_seed,\n",
        "    shuffle=False,\n",
        "    subset = 'training',\n",
        "    class_mode='binary')\n",
        "\n",
        "val_generator_vgg16 = train_datagen.flow_from_directory(\n",
        "    \"/content/drive/MyDrive/My_projects _and _datasets/Final_work/APTOS/Train/\",\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    seed = random_seed,\n",
        "    shuffle=False,\n",
        "    subset = 'validation',\n",
        "    class_mode='binary')\n",
        "test_datagen=ImageDataGenerator(rescale=1./255)\n",
        "test_generator_vgg16=test_datagen.flow_from_directory(\"/content/drive/MyDrive/My_projects _and _datasets/Final_work/APTOS/Test/\",\n",
        "                                                      target_size=(img_height, img_width),\n",
        "                                                          batch_size=batch_size, \n",
        "                                                          seed=random_seed,\n",
        "                                                          shuffle=False,\n",
        "                                                          class_mode='binary') # set as training data"
      ],
      "metadata": {
        "id": "BqmzI7eqNUvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b545e155-822c-4bc9-c270-4aba82bcdbe8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------Image Augmentation for VGG16--------------\n",
            "Found 2628 images belonging to 2 classes.\n",
            "Found 656 images belonging to 2 classes.\n",
            "Found 378 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_train_samples = len(train_generator_vgg16.filenames)\n",
        "nb_validation_samples = len(val_generator_vgg16.filenames)\n",
        "predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
        "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n",
        "\n",
        "nb_test_samples = len(test_generator_vgg16.filenames)\n",
        "predict_size_test = int(math.ceil(nb_test_samples / batch_size))\n",
        "print(nb_train_samples)\n",
        "print(nb_validation_samples)\n",
        "print(nb_test_samples)\n",
        "print(predict_size_train)\n",
        "print(predict_size_validation)\n",
        "print(predict_size_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pLTx9SBPRv1",
        "outputId": "26025cf4-6b43-4d6d-f600-698c2233080d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2628\n",
            "656\n",
            "378\n",
            "83\n",
            "21\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"VGG16\"\n",
        "model = VGG16(include_top=False, weights=\"imagenet\",pooling='avg',input_tensor=Input(shape=input_shape))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2WSFLNjbcx1",
        "outputId": "1882088b-2dd5-4a6d-b83a-15d5be00fcf8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 512)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "    print(i, layer.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHprI0cNc7CC",
        "outputId": "e84d4362-f9d6-463e-c754-c2b7421c2056"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_1\n",
            "1 block1_conv1\n",
            "2 block1_conv2\n",
            "3 block1_pool\n",
            "4 block2_conv1\n",
            "5 block2_conv2\n",
            "6 block2_pool\n",
            "7 block3_conv1\n",
            "8 block3_conv2\n",
            "9 block3_conv3\n",
            "10 block3_pool\n",
            "11 block4_conv1\n",
            "12 block4_conv2\n",
            "13 block4_conv3\n",
            "14 block4_pool\n",
            "15 block5_conv1\n",
            "16 block5_conv2\n",
            "17 block5_conv3\n",
            "18 block5_pool\n",
            "19 global_average_pooling2d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_input =model.input\n",
        "x1 = GlobalAveragePooling2D()(model.get_layer(\"block2_conv1\").output)  \n",
        "x2 = GlobalAveragePooling2D()(model.get_layer(\"block4_conv1\").output)  \n",
        "x3 = GlobalAveragePooling2D()(model.get_layer(\"block5_conv1\").output)  \n",
        "out= Concatenate()([x1,x2,x3])\n",
        "custom_vgg16_model = Model(image_input , out)\n",
        "custom_vgg16_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa2SHEaKfDtt",
        "outputId": "970fb891-d8d7-4454-88ee-000ab007a574"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 128)         0           ['block2_conv1[0][0]']           \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2 (Gl  (None, 512)         0           ['block4_conv1[0][0]']           \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_3 (Gl  (None, 512)         0           ['block5_conv1[0][0]']           \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1152)         0           ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'global_average_pooling2d_2[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'global_average_pooling2d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,995,072\n",
            "Trainable params: 9,995,072\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(custom_vgg16_model.layers):\n",
        "    print(i, layer.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9iIcczyfOmt",
        "outputId": "40e2a98a-1dc8-4c5a-ac2e-8bce98b06101"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_1\n",
            "1 block1_conv1\n",
            "2 block1_conv2\n",
            "3 block1_pool\n",
            "4 block2_conv1\n",
            "5 block2_conv2\n",
            "6 block2_pool\n",
            "7 block3_conv1\n",
            "8 block3_conv2\n",
            "9 block3_conv3\n",
            "10 block3_pool\n",
            "11 block4_conv1\n",
            "12 block4_conv2\n",
            "13 block4_conv3\n",
            "14 block4_pool\n",
            "15 block5_conv1\n",
            "16 global_average_pooling2d_1\n",
            "17 global_average_pooling2d_2\n",
            "18 global_average_pooling2d_3\n",
            "19 concatenate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in custom_vgg16_model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "custom_vgg16_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjle0MEifSDb",
        "outputId": "6088fa8f-2763-4a1c-8548-960a30d9198c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 224, 224, 64  1792        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 224, 224, 64  36928       ['block1_conv1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block1_pool (MaxPooling2D)     (None, 112, 112, 64  0           ['block1_conv2[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " block2_conv1 (Conv2D)          (None, 112, 112, 12  73856       ['block1_pool[0][0]']            \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_conv2 (Conv2D)          (None, 112, 112, 12  147584      ['block2_conv1[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 56, 56, 128)  0           ['block2_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv1 (Conv2D)          (None, 56, 56, 256)  295168      ['block2_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block3_conv2 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block3_conv3 (Conv2D)          (None, 56, 56, 256)  590080      ['block3_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 28, 28, 256)  0           ['block3_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv1 (Conv2D)          (None, 28, 28, 512)  1180160     ['block3_pool[0][0]']            \n",
            "                                                                                                  \n",
            " block4_conv2 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv1[0][0]']           \n",
            "                                                                                                  \n",
            " block4_conv3 (Conv2D)          (None, 28, 28, 512)  2359808     ['block4_conv2[0][0]']           \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 14, 14, 512)  0           ['block4_conv3[0][0]']           \n",
            "                                                                                                  \n",
            " block5_conv1 (Conv2D)          (None, 14, 14, 512)  2359808     ['block4_pool[0][0]']            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 128)         0           ['block2_conv1[0][0]']           \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2 (Gl  (None, 512)         0           ['block4_conv1[0][0]']           \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_3 (Gl  (None, 512)         0           ['block5_conv1[0][0]']           \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 1152)         0           ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'global_average_pooling2d_2[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'global_average_pooling2d_3[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,995,072\n",
            "Trainable params: 2,359,808\n",
            "Non-trainable params: 7,635,264\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bottleneck_features_train = custom_vgg16_model.predict_generator(train_generator_vgg16, predict_size_train)\n",
        "np.save('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/'+'bottleneck_features_train_'+model_name+'.npy', bottleneck_features_train)"
      ],
      "metadata": {
        "id": "L8UkoNKcf03_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bottleneck_features_validation = custom_vgg16_model.predict_generator(val_generator_vgg16, predict_size_validation)\n",
        "np.save('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/'+'bottleneck_features_validation_'+model_name+'.npy', bottleneck_features_validation)"
      ],
      "metadata": {
        "id": "2pPv2V9Vg_k9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bottleneck_features_test = custom_vgg16_model.predict_generator(test_generator_vgg16, predict_size_test)\n",
        "np.save(\"/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/\"+'bottleneck_features_test_'+model_name+'.npy', bottleneck_features_test)"
      ],
      "metadata": {
        "id": "PnFWMUPvNXoP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=np.load('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/bottleneck_features_train_VGG16.npy')\n",
        "validation_data=np.load('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/bottleneck_features_validation_VGG16.npy')\n",
        "test_data = np.load('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/bottleneck_features_test_VGG16.npy')"
      ],
      "metadata": {
        "id": "8SZO5UA9k_RW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(validation_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRAxoJ7vl40y",
        "outputId": "e3f2261e-744f-47d6-f66e-3145fb7d879b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2628, 1152)\n",
            "(656, 1152)\n",
            "(378, 1152)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels=train_generator_vgg16.classes\n",
        "train_labels=train_labels = to_categorical(train_labels, num_classes=2)\n",
        "validation_labels=val_generator_vgg16.classes\n",
        "validation_labels = to_categorical(validation_labels, num_classes=2)\n",
        "test_labels=test_generator_vgg16.classes\n",
        "test_labels=to_categorical(test_labels,num_classes=2)"
      ],
      "metadata": {
        "id": "Yrqp5buxmSIr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels.shape)\n",
        "print(validation_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMfhw3bpAIA",
        "outputId": "88c7902e-5a51-43f3-f6b4-10f33cc0e6cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2628, 2)\n",
            "(656, 2)\n",
            "(378, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(112,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2,activation='sigmoid'))\n",
        "adam_opt2=Adam(lr = 0.001, beta_1=0.6, beta_2=0.8, amsgrad=True)\n",
        "\n",
        "model.compile(optimizer=adam_opt2, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_data, train_labels,\n",
        "                    epochs=1000,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(validation_data, validation_labels),\n",
        "                    verbose= 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8etCfsTPpP9_",
        "outputId": "af730481-6bff-40f6-804a-8ed0a469ecf4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "83/83 [==============================] - 1s 8ms/step - loss: 0.6127 - accuracy: 0.7778 - val_loss: 0.3061 - val_accuracy: 0.9070\n",
            "Epoch 2/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.3441 - accuracy: 0.8786 - val_loss: 0.2186 - val_accuracy: 0.9405\n",
            "Epoch 3/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.3010 - accuracy: 0.9007 - val_loss: 0.2024 - val_accuracy: 0.9345\n",
            "Epoch 4/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.9106 - val_loss: 0.1686 - val_accuracy: 0.9451\n",
            "Epoch 5/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2580 - accuracy: 0.9182 - val_loss: 0.1599 - val_accuracy: 0.9497\n",
            "Epoch 6/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2458 - accuracy: 0.9163 - val_loss: 0.1942 - val_accuracy: 0.9543\n",
            "Epoch 7/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2426 - accuracy: 0.9182 - val_loss: 0.1510 - val_accuracy: 0.9558\n",
            "Epoch 8/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2389 - accuracy: 0.9243 - val_loss: 0.1316 - val_accuracy: 0.9604\n",
            "Epoch 9/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2323 - accuracy: 0.9243 - val_loss: 0.1361 - val_accuracy: 0.9543\n",
            "Epoch 10/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2256 - accuracy: 0.9288 - val_loss: 0.1539 - val_accuracy: 0.9543\n",
            "Epoch 11/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2129 - accuracy: 0.9326 - val_loss: 0.1198 - val_accuracy: 0.9619\n",
            "Epoch 12/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2095 - accuracy: 0.9353 - val_loss: 0.1250 - val_accuracy: 0.9588\n",
            "Epoch 13/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2081 - accuracy: 0.9349 - val_loss: 0.1544 - val_accuracy: 0.9604\n",
            "Epoch 14/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2150 - accuracy: 0.9368 - val_loss: 0.1243 - val_accuracy: 0.9619\n",
            "Epoch 15/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2074 - accuracy: 0.9380 - val_loss: 0.1089 - val_accuracy: 0.9665\n",
            "Epoch 16/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.2045 - accuracy: 0.9372 - val_loss: 0.1322 - val_accuracy: 0.9634\n",
            "Epoch 17/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9403 - val_loss: 0.1408 - val_accuracy: 0.9680\n",
            "Epoch 18/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1918 - accuracy: 0.9353 - val_loss: 0.1096 - val_accuracy: 0.9665\n",
            "Epoch 19/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1898 - accuracy: 0.9399 - val_loss: 0.1089 - val_accuracy: 0.9665\n",
            "Epoch 20/1000\n",
            "83/83 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9361 - val_loss: 0.0974 - val_accuracy: 0.9665\n",
            "Epoch 21/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1850 - accuracy: 0.9410 - val_loss: 0.0985 - val_accuracy: 0.9649\n",
            "Epoch 22/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1823 - accuracy: 0.9418 - val_loss: 0.1014 - val_accuracy: 0.9680\n",
            "Epoch 23/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1876 - accuracy: 0.9452 - val_loss: 0.1045 - val_accuracy: 0.9634\n",
            "Epoch 24/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1777 - accuracy: 0.9448 - val_loss: 0.0998 - val_accuracy: 0.9649\n",
            "Epoch 25/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1891 - accuracy: 0.9380 - val_loss: 0.0995 - val_accuracy: 0.9726\n",
            "Epoch 26/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.9444 - val_loss: 0.0933 - val_accuracy: 0.9665\n",
            "Epoch 27/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1826 - accuracy: 0.9467 - val_loss: 0.1019 - val_accuracy: 0.9604\n",
            "Epoch 28/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.9425 - val_loss: 0.0877 - val_accuracy: 0.9710\n",
            "Epoch 29/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9452 - val_loss: 0.0897 - val_accuracy: 0.9695\n",
            "Epoch 30/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1858 - accuracy: 0.9448 - val_loss: 0.0928 - val_accuracy: 0.9695\n",
            "Epoch 31/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1690 - accuracy: 0.9479 - val_loss: 0.1061 - val_accuracy: 0.9771\n",
            "Epoch 32/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.1758 - accuracy: 0.9441 - val_loss: 0.0915 - val_accuracy: 0.9680\n",
            "Epoch 33/1000\n",
            "83/83 [==============================] - 1s 11ms/step - loss: 0.1704 - accuracy: 0.9444 - val_loss: 0.0880 - val_accuracy: 0.9680\n",
            "Epoch 34/1000\n",
            "83/83 [==============================] - 1s 7ms/step - loss: 0.1721 - accuracy: 0.9494 - val_loss: 0.0893 - val_accuracy: 0.9680\n",
            "Epoch 35/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1794 - accuracy: 0.9463 - val_loss: 0.1008 - val_accuracy: 0.9741\n",
            "Epoch 36/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9509 - val_loss: 0.1427 - val_accuracy: 0.9604\n",
            "Epoch 37/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1658 - accuracy: 0.9448 - val_loss: 0.1924 - val_accuracy: 0.9360\n",
            "Epoch 38/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1784 - accuracy: 0.9441 - val_loss: 0.0892 - val_accuracy: 0.9695\n",
            "Epoch 39/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.9479 - val_loss: 0.0897 - val_accuracy: 0.9665\n",
            "Epoch 40/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1661 - accuracy: 0.9502 - val_loss: 0.0855 - val_accuracy: 0.9741\n",
            "Epoch 41/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1643 - accuracy: 0.9509 - val_loss: 0.1246 - val_accuracy: 0.9680\n",
            "Epoch 42/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1582 - accuracy: 0.9524 - val_loss: 0.0849 - val_accuracy: 0.9726\n",
            "Epoch 43/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1538 - accuracy: 0.9482 - val_loss: 0.0870 - val_accuracy: 0.9802\n",
            "Epoch 44/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9490 - val_loss: 0.0835 - val_accuracy: 0.9741\n",
            "Epoch 45/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1614 - accuracy: 0.9490 - val_loss: 0.0936 - val_accuracy: 0.9817\n",
            "Epoch 46/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1566 - accuracy: 0.9517 - val_loss: 0.0923 - val_accuracy: 0.9741\n",
            "Epoch 47/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.9517 - val_loss: 0.1261 - val_accuracy: 0.9665\n",
            "Epoch 48/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9490 - val_loss: 0.0922 - val_accuracy: 0.9787\n",
            "Epoch 49/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1548 - accuracy: 0.9566 - val_loss: 0.0824 - val_accuracy: 0.9695\n",
            "Epoch 50/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.9498 - val_loss: 0.0854 - val_accuracy: 0.9817\n",
            "Epoch 51/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1510 - accuracy: 0.9521 - val_loss: 0.0781 - val_accuracy: 0.9710\n",
            "Epoch 52/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9494 - val_loss: 0.0875 - val_accuracy: 0.9802\n",
            "Epoch 53/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1491 - accuracy: 0.9524 - val_loss: 0.0781 - val_accuracy: 0.9817\n",
            "Epoch 54/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9528 - val_loss: 0.0874 - val_accuracy: 0.9817\n",
            "Epoch 55/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1454 - accuracy: 0.9543 - val_loss: 0.0751 - val_accuracy: 0.9756\n",
            "Epoch 56/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1437 - accuracy: 0.9555 - val_loss: 0.0946 - val_accuracy: 0.9787\n",
            "Epoch 57/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9547 - val_loss: 0.0759 - val_accuracy: 0.9787\n",
            "Epoch 58/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1494 - accuracy: 0.9536 - val_loss: 0.0760 - val_accuracy: 0.9741\n",
            "Epoch 59/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1521 - accuracy: 0.9494 - val_loss: 0.0781 - val_accuracy: 0.9695\n",
            "Epoch 60/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1484 - accuracy: 0.9551 - val_loss: 0.0869 - val_accuracy: 0.9817\n",
            "Epoch 61/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1444 - accuracy: 0.9536 - val_loss: 0.0768 - val_accuracy: 0.9787\n",
            "Epoch 62/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1494 - accuracy: 0.9521 - val_loss: 0.1452 - val_accuracy: 0.9527\n",
            "Epoch 63/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1539 - accuracy: 0.9494 - val_loss: 0.0782 - val_accuracy: 0.9802\n",
            "Epoch 64/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1439 - accuracy: 0.9543 - val_loss: 0.0934 - val_accuracy: 0.9771\n",
            "Epoch 65/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9532 - val_loss: 0.0780 - val_accuracy: 0.9756\n",
            "Epoch 66/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9528 - val_loss: 0.0851 - val_accuracy: 0.9802\n",
            "Epoch 67/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9547 - val_loss: 0.0784 - val_accuracy: 0.9832\n",
            "Epoch 68/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9540 - val_loss: 0.0979 - val_accuracy: 0.9787\n",
            "Epoch 69/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9532 - val_loss: 0.1126 - val_accuracy: 0.9665\n",
            "Epoch 70/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1496 - accuracy: 0.9505 - val_loss: 0.0804 - val_accuracy: 0.9802\n",
            "Epoch 71/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1448 - accuracy: 0.9566 - val_loss: 0.0768 - val_accuracy: 0.9802\n",
            "Epoch 72/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1346 - accuracy: 0.9551 - val_loss: 0.0832 - val_accuracy: 0.9817\n",
            "Epoch 73/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1370 - accuracy: 0.9551 - val_loss: 0.0756 - val_accuracy: 0.9771\n",
            "Epoch 74/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9524 - val_loss: 0.0863 - val_accuracy: 0.9649\n",
            "Epoch 75/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1429 - accuracy: 0.9543 - val_loss: 0.0721 - val_accuracy: 0.9787\n",
            "Epoch 76/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9559 - val_loss: 0.0734 - val_accuracy: 0.9726\n",
            "Epoch 77/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9540 - val_loss: 0.0692 - val_accuracy: 0.9771\n",
            "Epoch 78/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.9559 - val_loss: 0.0694 - val_accuracy: 0.9817\n",
            "Epoch 79/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1446 - accuracy: 0.9521 - val_loss: 0.0749 - val_accuracy: 0.9802\n",
            "Epoch 80/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9547 - val_loss: 0.0702 - val_accuracy: 0.9848\n",
            "Epoch 81/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1415 - accuracy: 0.9570 - val_loss: 0.0692 - val_accuracy: 0.9817\n",
            "Epoch 82/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9581 - val_loss: 0.0889 - val_accuracy: 0.9787\n",
            "Epoch 83/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9574 - val_loss: 0.0950 - val_accuracy: 0.9756\n",
            "Epoch 84/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9581 - val_loss: 0.0766 - val_accuracy: 0.9817\n",
            "Epoch 85/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1335 - accuracy: 0.9574 - val_loss: 0.0679 - val_accuracy: 0.9832\n",
            "Epoch 86/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9536 - val_loss: 0.0746 - val_accuracy: 0.9832\n",
            "Epoch 87/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9570 - val_loss: 0.0704 - val_accuracy: 0.9756\n",
            "Epoch 88/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.9555 - val_loss: 0.0876 - val_accuracy: 0.9802\n",
            "Epoch 89/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9562 - val_loss: 0.0712 - val_accuracy: 0.9710\n",
            "Epoch 90/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9597 - val_loss: 0.0864 - val_accuracy: 0.9756\n",
            "Epoch 91/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1335 - accuracy: 0.9555 - val_loss: 0.0706 - val_accuracy: 0.9832\n",
            "Epoch 92/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9593 - val_loss: 0.0742 - val_accuracy: 0.9832\n",
            "Epoch 93/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9555 - val_loss: 0.0702 - val_accuracy: 0.9817\n",
            "Epoch 94/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9578 - val_loss: 0.0646 - val_accuracy: 0.9771\n",
            "Epoch 95/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9562 - val_loss: 0.0711 - val_accuracy: 0.9756\n",
            "Epoch 96/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9585 - val_loss: 0.0681 - val_accuracy: 0.9817\n",
            "Epoch 97/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1307 - accuracy: 0.9578 - val_loss: 0.1066 - val_accuracy: 0.9649\n",
            "Epoch 98/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9543 - val_loss: 0.0718 - val_accuracy: 0.9802\n",
            "Epoch 99/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9608 - val_loss: 0.0690 - val_accuracy: 0.9832\n",
            "Epoch 100/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1327 - accuracy: 0.9581 - val_loss: 0.0654 - val_accuracy: 0.9787\n",
            "Epoch 101/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9589 - val_loss: 0.0761 - val_accuracy: 0.9832\n",
            "Epoch 102/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1294 - accuracy: 0.9574 - val_loss: 0.0725 - val_accuracy: 0.9726\n",
            "Epoch 103/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1284 - accuracy: 0.9589 - val_loss: 0.0743 - val_accuracy: 0.9832\n",
            "Epoch 104/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1313 - accuracy: 0.9581 - val_loss: 0.1004 - val_accuracy: 0.9741\n",
            "Epoch 105/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1338 - accuracy: 0.9543 - val_loss: 0.0656 - val_accuracy: 0.9848\n",
            "Epoch 106/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9562 - val_loss: 0.0954 - val_accuracy: 0.9771\n",
            "Epoch 107/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1323 - accuracy: 0.9604 - val_loss: 0.0709 - val_accuracy: 0.9832\n",
            "Epoch 108/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.9593 - val_loss: 0.0874 - val_accuracy: 0.9817\n",
            "Epoch 109/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9600 - val_loss: 0.0678 - val_accuracy: 0.9832\n",
            "Epoch 110/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1297 - accuracy: 0.9570 - val_loss: 0.0724 - val_accuracy: 0.9832\n",
            "Epoch 111/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9547 - val_loss: 0.0694 - val_accuracy: 0.9787\n",
            "Epoch 112/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1313 - accuracy: 0.9562 - val_loss: 0.1131 - val_accuracy: 0.9619\n",
            "Epoch 113/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9589 - val_loss: 0.0672 - val_accuracy: 0.9832\n",
            "Epoch 114/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9581 - val_loss: 0.0638 - val_accuracy: 0.9802\n",
            "Epoch 115/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1299 - accuracy: 0.9597 - val_loss: 0.0693 - val_accuracy: 0.9832\n",
            "Epoch 116/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.9593 - val_loss: 0.0812 - val_accuracy: 0.9771\n",
            "Epoch 117/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1285 - accuracy: 0.9593 - val_loss: 0.0634 - val_accuracy: 0.9817\n",
            "Epoch 118/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9616 - val_loss: 0.1385 - val_accuracy: 0.9558\n",
            "Epoch 119/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1315 - accuracy: 0.9574 - val_loss: 0.0632 - val_accuracy: 0.9832\n",
            "Epoch 120/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 0.0701 - val_accuracy: 0.9848\n",
            "Epoch 121/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1213 - accuracy: 0.9600 - val_loss: 0.0715 - val_accuracy: 0.9817\n",
            "Epoch 122/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9608 - val_loss: 0.0775 - val_accuracy: 0.9817\n",
            "Epoch 123/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1263 - accuracy: 0.9604 - val_loss: 0.0670 - val_accuracy: 0.9832\n",
            "Epoch 124/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1185 - accuracy: 0.9631 - val_loss: 0.0950 - val_accuracy: 0.9726\n",
            "Epoch 125/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9593 - val_loss: 0.0755 - val_accuracy: 0.9680\n",
            "Epoch 126/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9585 - val_loss: 0.0753 - val_accuracy: 0.9817\n",
            "Epoch 127/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1277 - accuracy: 0.9612 - val_loss: 0.0717 - val_accuracy: 0.9802\n",
            "Epoch 128/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9597 - val_loss: 0.0697 - val_accuracy: 0.9832\n",
            "Epoch 129/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1236 - accuracy: 0.9612 - val_loss: 0.0816 - val_accuracy: 0.9756\n",
            "Epoch 130/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9631 - val_loss: 0.0631 - val_accuracy: 0.9817\n",
            "Epoch 131/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9627 - val_loss: 0.0609 - val_accuracy: 0.9863\n",
            "Epoch 132/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1141 - accuracy: 0.9631 - val_loss: 0.0661 - val_accuracy: 0.9848\n",
            "Epoch 133/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9612 - val_loss: 0.0628 - val_accuracy: 0.9817\n",
            "Epoch 134/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1288 - accuracy: 0.9574 - val_loss: 0.0674 - val_accuracy: 0.9832\n",
            "Epoch 135/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9661 - val_loss: 0.0629 - val_accuracy: 0.9817\n",
            "Epoch 136/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1183 - accuracy: 0.9646 - val_loss: 0.0718 - val_accuracy: 0.9817\n",
            "Epoch 137/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9585 - val_loss: 0.0642 - val_accuracy: 0.9832\n",
            "Epoch 138/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9593 - val_loss: 0.1470 - val_accuracy: 0.9512\n",
            "Epoch 139/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.1407 - accuracy: 0.9555 - val_loss: 0.0630 - val_accuracy: 0.9878\n",
            "Epoch 140/1000\n",
            "83/83 [==============================] - 1s 9ms/step - loss: 0.1305 - accuracy: 0.9642 - val_loss: 0.1053 - val_accuracy: 0.9634\n",
            "Epoch 141/1000\n",
            "83/83 [==============================] - 1s 7ms/step - loss: 0.1304 - accuracy: 0.9627 - val_loss: 0.0656 - val_accuracy: 0.9802\n",
            "Epoch 142/1000\n",
            "83/83 [==============================] - 1s 9ms/step - loss: 0.1286 - accuracy: 0.9604 - val_loss: 0.0799 - val_accuracy: 0.9787\n",
            "Epoch 143/1000\n",
            "83/83 [==============================] - 1s 8ms/step - loss: 0.1169 - accuracy: 0.9616 - val_loss: 0.0858 - val_accuracy: 0.9771\n",
            "Epoch 144/1000\n",
            "83/83 [==============================] - 1s 7ms/step - loss: 0.1221 - accuracy: 0.9581 - val_loss: 0.0687 - val_accuracy: 0.9832\n",
            "Epoch 145/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1174 - accuracy: 0.9593 - val_loss: 0.0633 - val_accuracy: 0.9756\n",
            "Epoch 146/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1190 - accuracy: 0.9597 - val_loss: 0.0693 - val_accuracy: 0.9817\n",
            "Epoch 147/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9623 - val_loss: 0.0692 - val_accuracy: 0.9832\n",
            "Epoch 148/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9585 - val_loss: 0.0626 - val_accuracy: 0.9832\n",
            "Epoch 149/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1209 - accuracy: 0.9612 - val_loss: 0.0594 - val_accuracy: 0.9832\n",
            "Epoch 150/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1162 - accuracy: 0.9631 - val_loss: 0.0645 - val_accuracy: 0.9741\n",
            "Epoch 151/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9646 - val_loss: 0.0958 - val_accuracy: 0.9741\n",
            "Epoch 152/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9650 - val_loss: 0.0683 - val_accuracy: 0.9802\n",
            "Epoch 153/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9627 - val_loss: 0.0721 - val_accuracy: 0.9817\n",
            "Epoch 154/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9581 - val_loss: 0.0684 - val_accuracy: 0.9832\n",
            "Epoch 155/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1277 - accuracy: 0.9547 - val_loss: 0.0642 - val_accuracy: 0.9832\n",
            "Epoch 156/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9608 - val_loss: 0.0592 - val_accuracy: 0.9848\n",
            "Epoch 157/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9619 - val_loss: 0.0679 - val_accuracy: 0.9817\n",
            "Epoch 158/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1186 - accuracy: 0.9646 - val_loss: 0.0921 - val_accuracy: 0.9695\n",
            "Epoch 159/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9616 - val_loss: 0.0695 - val_accuracy: 0.9832\n",
            "Epoch 160/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.9566 - val_loss: 0.0747 - val_accuracy: 0.9771\n",
            "Epoch 161/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9593 - val_loss: 0.0774 - val_accuracy: 0.9802\n",
            "Epoch 162/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1164 - accuracy: 0.9623 - val_loss: 0.0576 - val_accuracy: 0.9817\n",
            "Epoch 163/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9600 - val_loss: 0.0686 - val_accuracy: 0.9817\n",
            "Epoch 164/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9612 - val_loss: 0.0598 - val_accuracy: 0.9848\n",
            "Epoch 165/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9627 - val_loss: 0.0604 - val_accuracy: 0.9817\n",
            "Epoch 166/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1159 - accuracy: 0.9581 - val_loss: 0.0648 - val_accuracy: 0.9817\n",
            "Epoch 167/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9665 - val_loss: 0.0688 - val_accuracy: 0.9817\n",
            "Epoch 168/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1131 - accuracy: 0.9600 - val_loss: 0.0574 - val_accuracy: 0.9832\n",
            "Epoch 169/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9623 - val_loss: 0.0608 - val_accuracy: 0.9802\n",
            "Epoch 170/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9589 - val_loss: 0.0813 - val_accuracy: 0.9771\n",
            "Epoch 171/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9650 - val_loss: 0.0722 - val_accuracy: 0.9848\n",
            "Epoch 172/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9642 - val_loss: 0.0699 - val_accuracy: 0.9802\n",
            "Epoch 173/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9631 - val_loss: 0.0645 - val_accuracy: 0.9848\n",
            "Epoch 174/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9619 - val_loss: 0.0590 - val_accuracy: 0.9848\n",
            "Epoch 175/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9616 - val_loss: 0.0599 - val_accuracy: 0.9848\n",
            "Epoch 176/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9646 - val_loss: 0.0603 - val_accuracy: 0.9832\n",
            "Epoch 177/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1154 - accuracy: 0.9619 - val_loss: 0.0578 - val_accuracy: 0.9832\n",
            "Epoch 178/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1224 - accuracy: 0.9597 - val_loss: 0.0662 - val_accuracy: 0.9832\n",
            "Epoch 179/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1141 - accuracy: 0.9646 - val_loss: 0.0688 - val_accuracy: 0.9832\n",
            "Epoch 180/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9646 - val_loss: 0.0752 - val_accuracy: 0.9817\n",
            "Epoch 181/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9642 - val_loss: 0.0624 - val_accuracy: 0.9863\n",
            "Epoch 182/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9631 - val_loss: 0.0612 - val_accuracy: 0.9832\n",
            "Epoch 183/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9616 - val_loss: 0.0594 - val_accuracy: 0.9848\n",
            "Epoch 184/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9627 - val_loss: 0.0651 - val_accuracy: 0.9756\n",
            "Epoch 185/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9600 - val_loss: 0.0718 - val_accuracy: 0.9817\n",
            "Epoch 186/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9627 - val_loss: 0.0599 - val_accuracy: 0.9848\n",
            "Epoch 187/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1091 - accuracy: 0.9661 - val_loss: 0.0547 - val_accuracy: 0.9863\n",
            "Epoch 188/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9589 - val_loss: 0.0568 - val_accuracy: 0.9817\n",
            "Epoch 189/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1106 - accuracy: 0.9623 - val_loss: 0.0564 - val_accuracy: 0.9863\n",
            "Epoch 190/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9661 - val_loss: 0.0558 - val_accuracy: 0.9863\n",
            "Epoch 191/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1125 - accuracy: 0.9623 - val_loss: 0.0686 - val_accuracy: 0.9817\n",
            "Epoch 192/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9608 - val_loss: 0.0704 - val_accuracy: 0.9832\n",
            "Epoch 193/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1114 - accuracy: 0.9627 - val_loss: 0.0606 - val_accuracy: 0.9817\n",
            "Epoch 194/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9650 - val_loss: 0.0592 - val_accuracy: 0.9787\n",
            "Epoch 195/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9616 - val_loss: 0.0569 - val_accuracy: 0.9863\n",
            "Epoch 196/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1167 - accuracy: 0.9597 - val_loss: 0.0564 - val_accuracy: 0.9863\n",
            "Epoch 197/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1056 - accuracy: 0.9639 - val_loss: 0.0567 - val_accuracy: 0.9863\n",
            "Epoch 198/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9661 - val_loss: 0.0554 - val_accuracy: 0.9863\n",
            "Epoch 199/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1023 - accuracy: 0.9669 - val_loss: 0.0748 - val_accuracy: 0.9817\n",
            "Epoch 200/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9658 - val_loss: 0.0549 - val_accuracy: 0.9848\n",
            "Epoch 201/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 0.9646 - val_loss: 0.0592 - val_accuracy: 0.9771\n",
            "Epoch 202/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1169 - accuracy: 0.9593 - val_loss: 0.0876 - val_accuracy: 0.9741\n",
            "Epoch 203/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9616 - val_loss: 0.0684 - val_accuracy: 0.9817\n",
            "Epoch 204/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9639 - val_loss: 0.0591 - val_accuracy: 0.9863\n",
            "Epoch 205/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9631 - val_loss: 0.0622 - val_accuracy: 0.9817\n",
            "Epoch 206/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9654 - val_loss: 0.0560 - val_accuracy: 0.9863\n",
            "Epoch 207/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9604 - val_loss: 0.0954 - val_accuracy: 0.9634\n",
            "Epoch 208/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9521 - val_loss: 0.0608 - val_accuracy: 0.9848\n",
            "Epoch 209/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9658 - val_loss: 0.0577 - val_accuracy: 0.9787\n",
            "Epoch 210/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1131 - accuracy: 0.9669 - val_loss: 0.0560 - val_accuracy: 0.9832\n",
            "Epoch 211/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.9589 - val_loss: 0.0590 - val_accuracy: 0.9863\n",
            "Epoch 212/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9669 - val_loss: 0.0675 - val_accuracy: 0.9802\n",
            "Epoch 213/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9658 - val_loss: 0.0611 - val_accuracy: 0.9832\n",
            "Epoch 214/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9627 - val_loss: 0.0654 - val_accuracy: 0.9817\n",
            "Epoch 215/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1107 - accuracy: 0.9665 - val_loss: 0.0732 - val_accuracy: 0.9802\n",
            "Epoch 216/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9642 - val_loss: 0.0669 - val_accuracy: 0.9832\n",
            "Epoch 217/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1028 - accuracy: 0.9654 - val_loss: 0.0616 - val_accuracy: 0.9756\n",
            "Epoch 218/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1191 - accuracy: 0.9574 - val_loss: 0.0614 - val_accuracy: 0.9832\n",
            "Epoch 219/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9627 - val_loss: 0.0538 - val_accuracy: 0.9848\n",
            "Epoch 220/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 0.9646 - val_loss: 0.0565 - val_accuracy: 0.9863\n",
            "Epoch 221/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9703 - val_loss: 0.0606 - val_accuracy: 0.9863\n",
            "Epoch 222/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9658 - val_loss: 0.0666 - val_accuracy: 0.9787\n",
            "Epoch 223/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1046 - accuracy: 0.9658 - val_loss: 0.0716 - val_accuracy: 0.9787\n",
            "Epoch 224/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9642 - val_loss: 0.0577 - val_accuracy: 0.9848\n",
            "Epoch 225/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9688 - val_loss: 0.0627 - val_accuracy: 0.9832\n",
            "Epoch 226/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1073 - accuracy: 0.9654 - val_loss: 0.0899 - val_accuracy: 0.9680\n",
            "Epoch 227/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1080 - accuracy: 0.9642 - val_loss: 0.0840 - val_accuracy: 0.9741\n",
            "Epoch 228/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1039 - accuracy: 0.9665 - val_loss: 0.0538 - val_accuracy: 0.9863\n",
            "Epoch 229/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9658 - val_loss: 0.0629 - val_accuracy: 0.9863\n",
            "Epoch 230/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9635 - val_loss: 0.0736 - val_accuracy: 0.9756\n",
            "Epoch 231/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1129 - accuracy: 0.9619 - val_loss: 0.0623 - val_accuracy: 0.9832\n",
            "Epoch 232/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9658 - val_loss: 0.0613 - val_accuracy: 0.9802\n",
            "Epoch 233/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9658 - val_loss: 0.0529 - val_accuracy: 0.9848\n",
            "Epoch 234/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1068 - accuracy: 0.9654 - val_loss: 0.0741 - val_accuracy: 0.9710\n",
            "Epoch 235/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9551 - val_loss: 0.0733 - val_accuracy: 0.9771\n",
            "Epoch 236/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.1090 - accuracy: 0.9639 - val_loss: 0.0594 - val_accuracy: 0.9832\n",
            "Epoch 237/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1047 - accuracy: 0.9677 - val_loss: 0.0711 - val_accuracy: 0.9802\n",
            "Epoch 238/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9619 - val_loss: 0.0674 - val_accuracy: 0.9817\n",
            "Epoch 239/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9692 - val_loss: 0.0869 - val_accuracy: 0.9710\n",
            "Epoch 240/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1038 - accuracy: 0.9650 - val_loss: 0.0765 - val_accuracy: 0.9817\n",
            "Epoch 241/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9639 - val_loss: 0.0619 - val_accuracy: 0.9878\n",
            "Epoch 242/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9635 - val_loss: 0.0574 - val_accuracy: 0.9848\n",
            "Epoch 243/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9654 - val_loss: 0.0531 - val_accuracy: 0.9863\n",
            "Epoch 244/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9646 - val_loss: 0.0646 - val_accuracy: 0.9832\n",
            "Epoch 245/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1016 - accuracy: 0.9658 - val_loss: 0.0576 - val_accuracy: 0.9848\n",
            "Epoch 246/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1027 - accuracy: 0.9661 - val_loss: 0.0572 - val_accuracy: 0.9878\n",
            "Epoch 247/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9639 - val_loss: 0.0591 - val_accuracy: 0.9863\n",
            "Epoch 248/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.9684 - val_loss: 0.0548 - val_accuracy: 0.9848\n",
            "Epoch 249/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9688 - val_loss: 0.0906 - val_accuracy: 0.9695\n",
            "Epoch 250/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9654 - val_loss: 0.0551 - val_accuracy: 0.9848\n",
            "Epoch 251/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9665 - val_loss: 0.0553 - val_accuracy: 0.9817\n",
            "Epoch 252/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9627 - val_loss: 0.0721 - val_accuracy: 0.9832\n",
            "Epoch 253/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0989 - accuracy: 0.9673 - val_loss: 0.0559 - val_accuracy: 0.9848\n",
            "Epoch 254/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9658 - val_loss: 0.0598 - val_accuracy: 0.9832\n",
            "Epoch 255/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9658 - val_loss: 0.0512 - val_accuracy: 0.9848\n",
            "Epoch 256/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9673 - val_loss: 0.0545 - val_accuracy: 0.9848\n",
            "Epoch 257/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0978 - accuracy: 0.9661 - val_loss: 0.0635 - val_accuracy: 0.9817\n",
            "Epoch 258/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9631 - val_loss: 0.0505 - val_accuracy: 0.9863\n",
            "Epoch 259/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.9722 - val_loss: 0.0731 - val_accuracy: 0.9787\n",
            "Epoch 260/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0995 - accuracy: 0.9665 - val_loss: 0.0584 - val_accuracy: 0.9848\n",
            "Epoch 261/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0978 - accuracy: 0.9658 - val_loss: 0.0635 - val_accuracy: 0.9817\n",
            "Epoch 262/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9642 - val_loss: 0.0564 - val_accuracy: 0.9863\n",
            "Epoch 263/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9654 - val_loss: 0.0616 - val_accuracy: 0.9832\n",
            "Epoch 264/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9684 - val_loss: 0.0594 - val_accuracy: 0.9817\n",
            "Epoch 265/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0926 - accuracy: 0.9673 - val_loss: 0.0507 - val_accuracy: 0.9863\n",
            "Epoch 266/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0994 - accuracy: 0.9673 - val_loss: 0.0608 - val_accuracy: 0.9848\n",
            "Epoch 267/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1013 - accuracy: 0.9669 - val_loss: 0.0521 - val_accuracy: 0.9848\n",
            "Epoch 268/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9665 - val_loss: 0.0570 - val_accuracy: 0.9863\n",
            "Epoch 269/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0948 - accuracy: 0.9715 - val_loss: 0.0527 - val_accuracy: 0.9863\n",
            "Epoch 270/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9658 - val_loss: 0.0566 - val_accuracy: 0.9832\n",
            "Epoch 271/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9696 - val_loss: 0.0592 - val_accuracy: 0.9863\n",
            "Epoch 272/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.9661 - val_loss: 0.0508 - val_accuracy: 0.9817\n",
            "Epoch 273/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1009 - accuracy: 0.9692 - val_loss: 0.0607 - val_accuracy: 0.9832\n",
            "Epoch 274/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.9665 - val_loss: 0.0540 - val_accuracy: 0.9832\n",
            "Epoch 275/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9600 - val_loss: 0.0584 - val_accuracy: 0.9817\n",
            "Epoch 276/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9616 - val_loss: 0.0594 - val_accuracy: 0.9832\n",
            "Epoch 277/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0976 - accuracy: 0.9680 - val_loss: 0.0605 - val_accuracy: 0.9817\n",
            "Epoch 278/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1038 - accuracy: 0.9673 - val_loss: 0.0615 - val_accuracy: 0.9832\n",
            "Epoch 279/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9699 - val_loss: 0.0557 - val_accuracy: 0.9863\n",
            "Epoch 280/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0954 - accuracy: 0.9684 - val_loss: 0.0532 - val_accuracy: 0.9848\n",
            "Epoch 281/1000\n",
            "83/83 [==============================] - 1s 10ms/step - loss: 0.0970 - accuracy: 0.9680 - val_loss: 0.0535 - val_accuracy: 0.9848\n",
            "Epoch 282/1000\n",
            "83/83 [==============================] - 1s 11ms/step - loss: 0.0983 - accuracy: 0.9646 - val_loss: 0.0581 - val_accuracy: 0.9848\n",
            "Epoch 283/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9688 - val_loss: 0.0683 - val_accuracy: 0.9832\n",
            "Epoch 284/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0981 - accuracy: 0.9703 - val_loss: 0.0518 - val_accuracy: 0.9863\n",
            "Epoch 285/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9669 - val_loss: 0.0622 - val_accuracy: 0.9848\n",
            "Epoch 286/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0960 - accuracy: 0.9665 - val_loss: 0.0586 - val_accuracy: 0.9832\n",
            "Epoch 287/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9654 - val_loss: 0.0534 - val_accuracy: 0.9832\n",
            "Epoch 288/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9665 - val_loss: 0.0600 - val_accuracy: 0.9832\n",
            "Epoch 289/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9707 - val_loss: 0.0513 - val_accuracy: 0.9863\n",
            "Epoch 290/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.9661 - val_loss: 0.0626 - val_accuracy: 0.9848\n",
            "Epoch 291/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1021 - accuracy: 0.9650 - val_loss: 0.0521 - val_accuracy: 0.9863\n",
            "Epoch 292/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9692 - val_loss: 0.0952 - val_accuracy: 0.9649\n",
            "Epoch 293/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9669 - val_loss: 0.0506 - val_accuracy: 0.9848\n",
            "Epoch 294/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0966 - accuracy: 0.9669 - val_loss: 0.0496 - val_accuracy: 0.9863\n",
            "Epoch 295/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9665 - val_loss: 0.0526 - val_accuracy: 0.9832\n",
            "Epoch 296/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9677 - val_loss: 0.0879 - val_accuracy: 0.9726\n",
            "Epoch 297/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0979 - accuracy: 0.9680 - val_loss: 0.0715 - val_accuracy: 0.9756\n",
            "Epoch 298/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0914 - accuracy: 0.9688 - val_loss: 0.0506 - val_accuracy: 0.9863\n",
            "Epoch 299/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0998 - accuracy: 0.9635 - val_loss: 0.0792 - val_accuracy: 0.9832\n",
            "Epoch 300/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0894 - accuracy: 0.9696 - val_loss: 0.0911 - val_accuracy: 0.9695\n",
            "Epoch 301/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9673 - val_loss: 0.0527 - val_accuracy: 0.9802\n",
            "Epoch 302/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1030 - accuracy: 0.9677 - val_loss: 0.0623 - val_accuracy: 0.9863\n",
            "Epoch 303/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1049 - accuracy: 0.9646 - val_loss: 0.0562 - val_accuracy: 0.9832\n",
            "Epoch 304/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9661 - val_loss: 0.0619 - val_accuracy: 0.9832\n",
            "Epoch 305/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9616 - val_loss: 0.0676 - val_accuracy: 0.9848\n",
            "Epoch 306/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.1098 - accuracy: 0.9639 - val_loss: 0.0630 - val_accuracy: 0.9863\n",
            "Epoch 307/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 0.9639 - val_loss: 0.0549 - val_accuracy: 0.9848\n",
            "Epoch 308/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9646 - val_loss: 0.0566 - val_accuracy: 0.9802\n",
            "Epoch 309/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1038 - accuracy: 0.9680 - val_loss: 0.0618 - val_accuracy: 0.9863\n",
            "Epoch 310/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0979 - accuracy: 0.9707 - val_loss: 0.0613 - val_accuracy: 0.9756\n",
            "Epoch 311/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.9692 - val_loss: 0.0680 - val_accuracy: 0.9817\n",
            "Epoch 312/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9684 - val_loss: 0.0846 - val_accuracy: 0.9726\n",
            "Epoch 313/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0983 - accuracy: 0.9722 - val_loss: 0.0629 - val_accuracy: 0.9863\n",
            "Epoch 314/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0983 - accuracy: 0.9692 - val_loss: 0.0524 - val_accuracy: 0.9863\n",
            "Epoch 315/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9654 - val_loss: 0.0853 - val_accuracy: 0.9726\n",
            "Epoch 316/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0965 - accuracy: 0.9699 - val_loss: 0.0575 - val_accuracy: 0.9848\n",
            "Epoch 317/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9658 - val_loss: 0.0641 - val_accuracy: 0.9832\n",
            "Epoch 318/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0977 - accuracy: 0.9718 - val_loss: 0.0562 - val_accuracy: 0.9848\n",
            "Epoch 319/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0962 - accuracy: 0.9665 - val_loss: 0.0668 - val_accuracy: 0.9817\n",
            "Epoch 320/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0967 - accuracy: 0.9692 - val_loss: 0.0815 - val_accuracy: 0.9741\n",
            "Epoch 321/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1040 - accuracy: 0.9677 - val_loss: 0.0534 - val_accuracy: 0.9848\n",
            "Epoch 322/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9684 - val_loss: 0.0856 - val_accuracy: 0.9665\n",
            "Epoch 323/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1307 - accuracy: 0.9498 - val_loss: 0.0595 - val_accuracy: 0.9817\n",
            "Epoch 324/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9623 - val_loss: 0.0739 - val_accuracy: 0.9756\n",
            "Epoch 325/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9616 - val_loss: 0.0542 - val_accuracy: 0.9832\n",
            "Epoch 326/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9650 - val_loss: 0.0536 - val_accuracy: 0.9817\n",
            "Epoch 327/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0976 - accuracy: 0.9677 - val_loss: 0.0535 - val_accuracy: 0.9863\n",
            "Epoch 328/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1016 - accuracy: 0.9673 - val_loss: 0.0575 - val_accuracy: 0.9832\n",
            "Epoch 329/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9654 - val_loss: 0.0795 - val_accuracy: 0.9756\n",
            "Epoch 330/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1011 - accuracy: 0.9665 - val_loss: 0.0836 - val_accuracy: 0.9726\n",
            "Epoch 331/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0950 - accuracy: 0.9677 - val_loss: 0.0763 - val_accuracy: 0.9802\n",
            "Epoch 332/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9585 - val_loss: 0.0564 - val_accuracy: 0.9848\n",
            "Epoch 333/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0936 - accuracy: 0.9703 - val_loss: 0.0541 - val_accuracy: 0.9832\n",
            "Epoch 334/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1013 - accuracy: 0.9650 - val_loss: 0.0802 - val_accuracy: 0.9741\n",
            "Epoch 335/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.9688 - val_loss: 0.0665 - val_accuracy: 0.9832\n",
            "Epoch 336/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0973 - accuracy: 0.9654 - val_loss: 0.0531 - val_accuracy: 0.9863\n",
            "Epoch 337/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9692 - val_loss: 0.0649 - val_accuracy: 0.9817\n",
            "Epoch 338/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9688 - val_loss: 0.0957 - val_accuracy: 0.9634\n",
            "Epoch 339/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0994 - accuracy: 0.9680 - val_loss: 0.0536 - val_accuracy: 0.9832\n",
            "Epoch 340/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0926 - accuracy: 0.9684 - val_loss: 0.0636 - val_accuracy: 0.9832\n",
            "Epoch 341/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9715 - val_loss: 0.0603 - val_accuracy: 0.9863\n",
            "Epoch 342/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0937 - accuracy: 0.9684 - val_loss: 0.0502 - val_accuracy: 0.9863\n",
            "Epoch 343/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9696 - val_loss: 0.0509 - val_accuracy: 0.9863\n",
            "Epoch 344/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0965 - accuracy: 0.9684 - val_loss: 0.0524 - val_accuracy: 0.9848\n",
            "Epoch 345/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.9673 - val_loss: 0.0796 - val_accuracy: 0.9756\n",
            "Epoch 346/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0980 - accuracy: 0.9669 - val_loss: 0.0693 - val_accuracy: 0.9726\n",
            "Epoch 347/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0882 - accuracy: 0.9737 - val_loss: 0.0502 - val_accuracy: 0.9832\n",
            "Epoch 348/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0907 - accuracy: 0.9707 - val_loss: 0.0533 - val_accuracy: 0.9863\n",
            "Epoch 349/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0930 - accuracy: 0.9688 - val_loss: 0.0497 - val_accuracy: 0.9848\n",
            "Epoch 350/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1027 - accuracy: 0.9658 - val_loss: 0.0660 - val_accuracy: 0.9817\n",
            "Epoch 351/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0918 - accuracy: 0.9715 - val_loss: 0.0905 - val_accuracy: 0.9665\n",
            "Epoch 352/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.9707 - val_loss: 0.0526 - val_accuracy: 0.9848\n",
            "Epoch 353/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0954 - accuracy: 0.9677 - val_loss: 0.0532 - val_accuracy: 0.9863\n",
            "Epoch 354/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0898 - accuracy: 0.9707 - val_loss: 0.0528 - val_accuracy: 0.9863\n",
            "Epoch 355/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9684 - val_loss: 0.0597 - val_accuracy: 0.9817\n",
            "Epoch 356/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9692 - val_loss: 0.0692 - val_accuracy: 0.9787\n",
            "Epoch 357/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9730 - val_loss: 0.0687 - val_accuracy: 0.9817\n",
            "Epoch 358/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9669 - val_loss: 0.0508 - val_accuracy: 0.9863\n",
            "Epoch 359/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0882 - accuracy: 0.9715 - val_loss: 0.0484 - val_accuracy: 0.9863\n",
            "Epoch 360/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9658 - val_loss: 0.0676 - val_accuracy: 0.9817\n",
            "Epoch 361/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0928 - accuracy: 0.9692 - val_loss: 0.0633 - val_accuracy: 0.9787\n",
            "Epoch 362/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0861 - accuracy: 0.9680 - val_loss: 0.0515 - val_accuracy: 0.9848\n",
            "Epoch 363/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 0.9680 - val_loss: 0.0530 - val_accuracy: 0.9863\n",
            "Epoch 364/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.9715 - val_loss: 0.0564 - val_accuracy: 0.9832\n",
            "Epoch 365/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9726 - val_loss: 0.0506 - val_accuracy: 0.9848\n",
            "Epoch 366/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9715 - val_loss: 0.0505 - val_accuracy: 0.9863\n",
            "Epoch 367/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0924 - accuracy: 0.9715 - val_loss: 0.0633 - val_accuracy: 0.9817\n",
            "Epoch 368/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0850 - accuracy: 0.9722 - val_loss: 0.0645 - val_accuracy: 0.9817\n",
            "Epoch 369/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9593 - val_loss: 0.0541 - val_accuracy: 0.9832\n",
            "Epoch 370/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9669 - val_loss: 0.0721 - val_accuracy: 0.9741\n",
            "Epoch 371/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9673 - val_loss: 0.0534 - val_accuracy: 0.9848\n",
            "Epoch 372/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9696 - val_loss: 0.0963 - val_accuracy: 0.9680\n",
            "Epoch 373/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0931 - accuracy: 0.9688 - val_loss: 0.0719 - val_accuracy: 0.9756\n",
            "Epoch 374/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0933 - accuracy: 0.9680 - val_loss: 0.0511 - val_accuracy: 0.9817\n",
            "Epoch 375/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0967 - accuracy: 0.9680 - val_loss: 0.0569 - val_accuracy: 0.9817\n",
            "Epoch 376/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0951 - accuracy: 0.9661 - val_loss: 0.0523 - val_accuracy: 0.9848\n",
            "Epoch 377/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0924 - accuracy: 0.9684 - val_loss: 0.0520 - val_accuracy: 0.9863\n",
            "Epoch 378/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0904 - accuracy: 0.9699 - val_loss: 0.0500 - val_accuracy: 0.9848\n",
            "Epoch 379/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0876 - accuracy: 0.9718 - val_loss: 0.0609 - val_accuracy: 0.9802\n",
            "Epoch 380/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0914 - accuracy: 0.9692 - val_loss: 0.0503 - val_accuracy: 0.9863\n",
            "Epoch 381/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9718 - val_loss: 0.0662 - val_accuracy: 0.9817\n",
            "Epoch 382/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0865 - accuracy: 0.9722 - val_loss: 0.0528 - val_accuracy: 0.9848\n",
            "Epoch 383/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0866 - accuracy: 0.9692 - val_loss: 0.0556 - val_accuracy: 0.9832\n",
            "Epoch 384/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 0.9699 - val_loss: 0.0622 - val_accuracy: 0.9817\n",
            "Epoch 385/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9718 - val_loss: 0.0503 - val_accuracy: 0.9832\n",
            "Epoch 386/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0941 - accuracy: 0.9684 - val_loss: 0.0598 - val_accuracy: 0.9848\n",
            "Epoch 387/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0877 - accuracy: 0.9669 - val_loss: 0.0568 - val_accuracy: 0.9802\n",
            "Epoch 388/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0827 - accuracy: 0.9715 - val_loss: 0.0550 - val_accuracy: 0.9832\n",
            "Epoch 389/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0877 - accuracy: 0.9680 - val_loss: 0.0587 - val_accuracy: 0.9832\n",
            "Epoch 390/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0976 - accuracy: 0.9654 - val_loss: 0.0550 - val_accuracy: 0.9802\n",
            "Epoch 391/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.1030 - accuracy: 0.9661 - val_loss: 0.0690 - val_accuracy: 0.9787\n",
            "Epoch 392/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0941 - accuracy: 0.9684 - val_loss: 0.0730 - val_accuracy: 0.9741\n",
            "Epoch 393/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0857 - accuracy: 0.9722 - val_loss: 0.0559 - val_accuracy: 0.9832\n",
            "Epoch 394/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0863 - accuracy: 0.9726 - val_loss: 0.0519 - val_accuracy: 0.9832\n",
            "Epoch 395/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9722 - val_loss: 0.0602 - val_accuracy: 0.9848\n",
            "Epoch 396/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9722 - val_loss: 0.0602 - val_accuracy: 0.9832\n",
            "Epoch 397/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0839 - accuracy: 0.9726 - val_loss: 0.0553 - val_accuracy: 0.9848\n",
            "Epoch 398/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9711 - val_loss: 0.0512 - val_accuracy: 0.9832\n",
            "Epoch 399/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0821 - accuracy: 0.9711 - val_loss: 0.0615 - val_accuracy: 0.9832\n",
            "Epoch 400/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9703 - val_loss: 0.0495 - val_accuracy: 0.9848\n",
            "Epoch 401/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0860 - accuracy: 0.9692 - val_loss: 0.0601 - val_accuracy: 0.9848\n",
            "Epoch 402/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0878 - accuracy: 0.9699 - val_loss: 0.0718 - val_accuracy: 0.9771\n",
            "Epoch 403/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0841 - accuracy: 0.9669 - val_loss: 0.0583 - val_accuracy: 0.9878\n",
            "Epoch 404/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0858 - accuracy: 0.9730 - val_loss: 0.0594 - val_accuracy: 0.9848\n",
            "Epoch 405/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9711 - val_loss: 0.0742 - val_accuracy: 0.9771\n",
            "Epoch 406/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.9722 - val_loss: 0.0586 - val_accuracy: 0.9848\n",
            "Epoch 407/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9707 - val_loss: 0.0586 - val_accuracy: 0.9802\n",
            "Epoch 408/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0894 - accuracy: 0.9692 - val_loss: 0.0704 - val_accuracy: 0.9741\n",
            "Epoch 409/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0824 - accuracy: 0.9722 - val_loss: 0.0493 - val_accuracy: 0.9832\n",
            "Epoch 410/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.9715 - val_loss: 0.0535 - val_accuracy: 0.9848\n",
            "Epoch 411/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0839 - accuracy: 0.9737 - val_loss: 0.0492 - val_accuracy: 0.9848\n",
            "Epoch 412/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0835 - accuracy: 0.9707 - val_loss: 0.0605 - val_accuracy: 0.9787\n",
            "Epoch 413/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9764 - val_loss: 0.0583 - val_accuracy: 0.9848\n",
            "Epoch 414/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 0.9715 - val_loss: 0.0612 - val_accuracy: 0.9817\n",
            "Epoch 415/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0816 - accuracy: 0.9722 - val_loss: 0.0632 - val_accuracy: 0.9802\n",
            "Epoch 416/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9699 - val_loss: 0.0784 - val_accuracy: 0.9726\n",
            "Epoch 417/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9680 - val_loss: 0.0994 - val_accuracy: 0.9634\n",
            "Epoch 418/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0822 - accuracy: 0.9722 - val_loss: 0.0495 - val_accuracy: 0.9832\n",
            "Epoch 419/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0981 - accuracy: 0.9692 - val_loss: 0.0639 - val_accuracy: 0.9771\n",
            "Epoch 420/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9703 - val_loss: 0.0548 - val_accuracy: 0.9832\n",
            "Epoch 421/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0837 - accuracy: 0.9707 - val_loss: 0.0733 - val_accuracy: 0.9741\n",
            "Epoch 422/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0817 - accuracy: 0.9707 - val_loss: 0.0519 - val_accuracy: 0.9832\n",
            "Epoch 423/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9715 - val_loss: 0.0827 - val_accuracy: 0.9695\n",
            "Epoch 424/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9718 - val_loss: 0.0698 - val_accuracy: 0.9771\n",
            "Epoch 425/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9726 - val_loss: 0.0599 - val_accuracy: 0.9832\n",
            "Epoch 426/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9718 - val_loss: 0.0553 - val_accuracy: 0.9848\n",
            "Epoch 427/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0891 - accuracy: 0.9722 - val_loss: 0.0985 - val_accuracy: 0.9665\n",
            "Epoch 428/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0923 - accuracy: 0.9711 - val_loss: 0.0610 - val_accuracy: 0.9802\n",
            "Epoch 429/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0812 - accuracy: 0.9741 - val_loss: 0.0666 - val_accuracy: 0.9832\n",
            "Epoch 430/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9711 - val_loss: 0.0839 - val_accuracy: 0.9726\n",
            "Epoch 431/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0896 - accuracy: 0.9688 - val_loss: 0.0739 - val_accuracy: 0.9710\n",
            "Epoch 432/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0922 - accuracy: 0.9696 - val_loss: 0.0611 - val_accuracy: 0.9802\n",
            "Epoch 433/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0887 - accuracy: 0.9734 - val_loss: 0.0634 - val_accuracy: 0.9832\n",
            "Epoch 434/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0824 - accuracy: 0.9734 - val_loss: 0.0614 - val_accuracy: 0.9817\n",
            "Epoch 435/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0839 - accuracy: 0.9718 - val_loss: 0.0486 - val_accuracy: 0.9863\n",
            "Epoch 436/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9680 - val_loss: 0.0556 - val_accuracy: 0.9863\n",
            "Epoch 437/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.9772 - val_loss: 0.0603 - val_accuracy: 0.9848\n",
            "Epoch 438/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0794 - accuracy: 0.9703 - val_loss: 0.0867 - val_accuracy: 0.9726\n",
            "Epoch 439/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 0.9692 - val_loss: 0.0718 - val_accuracy: 0.9771\n",
            "Epoch 440/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9753 - val_loss: 0.0485 - val_accuracy: 0.9863\n",
            "Epoch 441/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0823 - accuracy: 0.9741 - val_loss: 0.1169 - val_accuracy: 0.9543\n",
            "Epoch 442/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0854 - accuracy: 0.9730 - val_loss: 0.0583 - val_accuracy: 0.9802\n",
            "Epoch 443/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0811 - accuracy: 0.9722 - val_loss: 0.0493 - val_accuracy: 0.9863\n",
            "Epoch 444/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9734 - val_loss: 0.0503 - val_accuracy: 0.9848\n",
            "Epoch 445/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9760 - val_loss: 0.0512 - val_accuracy: 0.9863\n",
            "Epoch 446/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0887 - accuracy: 0.9684 - val_loss: 0.0624 - val_accuracy: 0.9787\n",
            "Epoch 447/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9699 - val_loss: 0.0526 - val_accuracy: 0.9832\n",
            "Epoch 448/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9737 - val_loss: 0.0665 - val_accuracy: 0.9802\n",
            "Epoch 449/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0799 - accuracy: 0.9737 - val_loss: 0.0503 - val_accuracy: 0.9832\n",
            "Epoch 450/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0882 - accuracy: 0.9692 - val_loss: 0.0578 - val_accuracy: 0.9817\n",
            "Epoch 451/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0703 - accuracy: 0.9741 - val_loss: 0.0482 - val_accuracy: 0.9863\n",
            "Epoch 452/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0815 - accuracy: 0.9722 - val_loss: 0.0539 - val_accuracy: 0.9832\n",
            "Epoch 453/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9726 - val_loss: 0.0493 - val_accuracy: 0.9863\n",
            "Epoch 454/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0788 - accuracy: 0.9772 - val_loss: 0.0623 - val_accuracy: 0.9787\n",
            "Epoch 455/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0825 - accuracy: 0.9722 - val_loss: 0.0662 - val_accuracy: 0.9771\n",
            "Epoch 456/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0824 - accuracy: 0.9726 - val_loss: 0.0512 - val_accuracy: 0.9832\n",
            "Epoch 457/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9741 - val_loss: 0.0625 - val_accuracy: 0.9832\n",
            "Epoch 458/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0763 - accuracy: 0.9707 - val_loss: 0.1262 - val_accuracy: 0.9573\n",
            "Epoch 459/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0979 - accuracy: 0.9741 - val_loss: 0.0631 - val_accuracy: 0.9848\n",
            "Epoch 460/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0887 - accuracy: 0.9715 - val_loss: 0.0541 - val_accuracy: 0.9832\n",
            "Epoch 461/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0846 - accuracy: 0.9722 - val_loss: 0.0581 - val_accuracy: 0.9832\n",
            "Epoch 462/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0827 - accuracy: 0.9745 - val_loss: 0.0517 - val_accuracy: 0.9848\n",
            "Epoch 463/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0929 - accuracy: 0.9707 - val_loss: 0.0553 - val_accuracy: 0.9848\n",
            "Epoch 464/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9730 - val_loss: 0.1243 - val_accuracy: 0.9543\n",
            "Epoch 465/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0968 - accuracy: 0.9696 - val_loss: 0.0553 - val_accuracy: 0.9832\n",
            "Epoch 466/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9749 - val_loss: 0.0600 - val_accuracy: 0.9817\n",
            "Epoch 467/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0717 - accuracy: 0.9737 - val_loss: 0.0605 - val_accuracy: 0.9817\n",
            "Epoch 468/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9726 - val_loss: 0.0535 - val_accuracy: 0.9848\n",
            "Epoch 469/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9745 - val_loss: 0.0614 - val_accuracy: 0.9817\n",
            "Epoch 470/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0808 - accuracy: 0.9764 - val_loss: 0.0668 - val_accuracy: 0.9817\n",
            "Epoch 471/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9734 - val_loss: 0.0637 - val_accuracy: 0.9817\n",
            "Epoch 472/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0759 - accuracy: 0.9726 - val_loss: 0.0497 - val_accuracy: 0.9848\n",
            "Epoch 473/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.0570 - val_accuracy: 0.9817\n",
            "Epoch 474/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.9749 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
            "Epoch 475/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9745 - val_loss: 0.0539 - val_accuracy: 0.9817\n",
            "Epoch 476/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9756 - val_loss: 0.0486 - val_accuracy: 0.9832\n",
            "Epoch 477/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9734 - val_loss: 0.0577 - val_accuracy: 0.9817\n",
            "Epoch 478/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0740 - accuracy: 0.9753 - val_loss: 0.0547 - val_accuracy: 0.9848\n",
            "Epoch 479/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 0.9749 - val_loss: 0.0749 - val_accuracy: 0.9695\n",
            "Epoch 480/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.9779 - val_loss: 0.0496 - val_accuracy: 0.9848\n",
            "Epoch 481/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0780 - accuracy: 0.9756 - val_loss: 0.0582 - val_accuracy: 0.9802\n",
            "Epoch 482/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0827 - accuracy: 0.9722 - val_loss: 0.0483 - val_accuracy: 0.9863\n",
            "Epoch 483/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0790 - accuracy: 0.9734 - val_loss: 0.0529 - val_accuracy: 0.9832\n",
            "Epoch 484/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.9715 - val_loss: 0.0500 - val_accuracy: 0.9832\n",
            "Epoch 485/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9718 - val_loss: 0.0581 - val_accuracy: 0.9848\n",
            "Epoch 486/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9756 - val_loss: 0.0488 - val_accuracy: 0.9848\n",
            "Epoch 487/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0740 - accuracy: 0.9760 - val_loss: 0.0520 - val_accuracy: 0.9832\n",
            "Epoch 488/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9753 - val_loss: 0.0515 - val_accuracy: 0.9832\n",
            "Epoch 489/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0779 - accuracy: 0.9737 - val_loss: 0.0797 - val_accuracy: 0.9710\n",
            "Epoch 490/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0725 - accuracy: 0.9726 - val_loss: 0.0872 - val_accuracy: 0.9726\n",
            "Epoch 491/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9726 - val_loss: 0.0547 - val_accuracy: 0.9832\n",
            "Epoch 492/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0790 - accuracy: 0.9737 - val_loss: 0.0692 - val_accuracy: 0.9741\n",
            "Epoch 493/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0802 - accuracy: 0.9722 - val_loss: 0.0512 - val_accuracy: 0.9832\n",
            "Epoch 494/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0778 - accuracy: 0.9756 - val_loss: 0.1108 - val_accuracy: 0.9588\n",
            "Epoch 495/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9745 - val_loss: 0.0557 - val_accuracy: 0.9848\n",
            "Epoch 496/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9753 - val_loss: 0.0744 - val_accuracy: 0.9710\n",
            "Epoch 497/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9703 - val_loss: 0.0519 - val_accuracy: 0.9863\n",
            "Epoch 498/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0711 - accuracy: 0.9764 - val_loss: 0.0553 - val_accuracy: 0.9802\n",
            "Epoch 499/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0770 - accuracy: 0.9745 - val_loss: 0.0669 - val_accuracy: 0.9787\n",
            "Epoch 500/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0758 - accuracy: 0.9756 - val_loss: 0.0498 - val_accuracy: 0.9832\n",
            "Epoch 501/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0887 - accuracy: 0.9707 - val_loss: 0.0491 - val_accuracy: 0.9817\n",
            "Epoch 502/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0808 - accuracy: 0.9718 - val_loss: 0.0490 - val_accuracy: 0.9832\n",
            "Epoch 503/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9726 - val_loss: 0.0717 - val_accuracy: 0.9741\n",
            "Epoch 504/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0804 - accuracy: 0.9703 - val_loss: 0.0674 - val_accuracy: 0.9726\n",
            "Epoch 505/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0838 - accuracy: 0.9722 - val_loss: 0.0533 - val_accuracy: 0.9832\n",
            "Epoch 506/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0864 - accuracy: 0.9684 - val_loss: 0.0556 - val_accuracy: 0.9848\n",
            "Epoch 507/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9711 - val_loss: 0.0499 - val_accuracy: 0.9832\n",
            "Epoch 508/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0707 - accuracy: 0.9783 - val_loss: 0.0498 - val_accuracy: 0.9832\n",
            "Epoch 509/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.9707 - val_loss: 0.0618 - val_accuracy: 0.9771\n",
            "Epoch 510/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9734 - val_loss: 0.0554 - val_accuracy: 0.9817\n",
            "Epoch 511/1000\n",
            "83/83 [==============================] - 1s 8ms/step - loss: 0.0781 - accuracy: 0.9745 - val_loss: 0.0591 - val_accuracy: 0.9802\n",
            "Epoch 512/1000\n",
            "83/83 [==============================] - 1s 12ms/step - loss: 0.0752 - accuracy: 0.9715 - val_loss: 0.0503 - val_accuracy: 0.9832\n",
            "Epoch 513/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0817 - accuracy: 0.9741 - val_loss: 0.0561 - val_accuracy: 0.9817\n",
            "Epoch 514/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0716 - accuracy: 0.9745 - val_loss: 0.0703 - val_accuracy: 0.9787\n",
            "Epoch 515/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 0.9787 - val_loss: 0.0538 - val_accuracy: 0.9832\n",
            "Epoch 516/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0776 - accuracy: 0.9711 - val_loss: 0.0523 - val_accuracy: 0.9863\n",
            "Epoch 517/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0724 - accuracy: 0.9772 - val_loss: 0.0573 - val_accuracy: 0.9848\n",
            "Epoch 518/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0737 - accuracy: 0.9756 - val_loss: 0.0559 - val_accuracy: 0.9802\n",
            "Epoch 519/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0989 - accuracy: 0.9642 - val_loss: 0.0585 - val_accuracy: 0.9817\n",
            "Epoch 520/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9680 - val_loss: 0.0497 - val_accuracy: 0.9802\n",
            "Epoch 521/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0796 - accuracy: 0.9730 - val_loss: 0.0533 - val_accuracy: 0.9832\n",
            "Epoch 522/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0806 - accuracy: 0.9734 - val_loss: 0.0542 - val_accuracy: 0.9848\n",
            "Epoch 523/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0786 - accuracy: 0.9730 - val_loss: 0.0487 - val_accuracy: 0.9848\n",
            "Epoch 524/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0768 - accuracy: 0.9718 - val_loss: 0.0715 - val_accuracy: 0.9741\n",
            "Epoch 525/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0777 - accuracy: 0.9734 - val_loss: 0.0586 - val_accuracy: 0.9771\n",
            "Epoch 526/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0732 - accuracy: 0.9753 - val_loss: 0.0751 - val_accuracy: 0.9741\n",
            "Epoch 527/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9737 - val_loss: 0.0820 - val_accuracy: 0.9695\n",
            "Epoch 528/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0741 - accuracy: 0.9730 - val_loss: 0.0571 - val_accuracy: 0.9817\n",
            "Epoch 529/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0758 - accuracy: 0.9715 - val_loss: 0.0518 - val_accuracy: 0.9832\n",
            "Epoch 530/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0724 - accuracy: 0.9760 - val_loss: 0.0630 - val_accuracy: 0.9848\n",
            "Epoch 531/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9722 - val_loss: 0.0510 - val_accuracy: 0.9832\n",
            "Epoch 532/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9734 - val_loss: 0.0496 - val_accuracy: 0.9832\n",
            "Epoch 533/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0761 - accuracy: 0.9730 - val_loss: 0.0516 - val_accuracy: 0.9848\n",
            "Epoch 534/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0735 - accuracy: 0.9779 - val_loss: 0.0496 - val_accuracy: 0.9848\n",
            "Epoch 535/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0744 - accuracy: 0.9741 - val_loss: 0.0624 - val_accuracy: 0.9848\n",
            "Epoch 536/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9760 - val_loss: 0.0520 - val_accuracy: 0.9848\n",
            "Epoch 537/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0766 - accuracy: 0.9726 - val_loss: 0.0482 - val_accuracy: 0.9848\n",
            "Epoch 538/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9707 - val_loss: 0.0549 - val_accuracy: 0.9832\n",
            "Epoch 539/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0719 - accuracy: 0.9741 - val_loss: 0.0672 - val_accuracy: 0.9832\n",
            "Epoch 540/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9756 - val_loss: 0.0587 - val_accuracy: 0.9832\n",
            "Epoch 541/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0767 - accuracy: 0.9711 - val_loss: 0.0530 - val_accuracy: 0.9848\n",
            "Epoch 542/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0761 - accuracy: 0.9756 - val_loss: 0.0539 - val_accuracy: 0.9832\n",
            "Epoch 543/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0733 - accuracy: 0.9730 - val_loss: 0.0530 - val_accuracy: 0.9832\n",
            "Epoch 544/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0697 - accuracy: 0.9749 - val_loss: 0.0631 - val_accuracy: 0.9802\n",
            "Epoch 545/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0793 - accuracy: 0.9734 - val_loss: 0.0808 - val_accuracy: 0.9680\n",
            "Epoch 546/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9760 - val_loss: 0.0633 - val_accuracy: 0.9832\n",
            "Epoch 547/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0797 - accuracy: 0.9760 - val_loss: 0.0563 - val_accuracy: 0.9848\n",
            "Epoch 548/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0729 - accuracy: 0.9768 - val_loss: 0.0810 - val_accuracy: 0.9710\n",
            "Epoch 549/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.0697 - val_accuracy: 0.9787\n",
            "Epoch 550/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.9745 - val_loss: 0.0595 - val_accuracy: 0.9848\n",
            "Epoch 551/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 0.9737 - val_loss: 0.0530 - val_accuracy: 0.9832\n",
            "Epoch 552/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 0.9734 - val_loss: 0.0483 - val_accuracy: 0.9863\n",
            "Epoch 553/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9764 - val_loss: 0.0845 - val_accuracy: 0.9695\n",
            "Epoch 554/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.9745 - val_loss: 0.0539 - val_accuracy: 0.9832\n",
            "Epoch 555/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0646 - accuracy: 0.9795 - val_loss: 0.0523 - val_accuracy: 0.9848\n",
            "Epoch 556/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9756 - val_loss: 0.0648 - val_accuracy: 0.9802\n",
            "Epoch 557/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0704 - accuracy: 0.9760 - val_loss: 0.0508 - val_accuracy: 0.9832\n",
            "Epoch 558/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9760 - val_loss: 0.0542 - val_accuracy: 0.9817\n",
            "Epoch 559/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 0.9753 - val_loss: 0.0541 - val_accuracy: 0.9832\n",
            "Epoch 560/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0695 - accuracy: 0.9764 - val_loss: 0.0625 - val_accuracy: 0.9832\n",
            "Epoch 561/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.0515 - val_accuracy: 0.9848\n",
            "Epoch 562/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9783 - val_loss: 0.0672 - val_accuracy: 0.9741\n",
            "Epoch 563/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0764 - accuracy: 0.9749 - val_loss: 0.0555 - val_accuracy: 0.9832\n",
            "Epoch 564/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0713 - accuracy: 0.9745 - val_loss: 0.0706 - val_accuracy: 0.9771\n",
            "Epoch 565/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9772 - val_loss: 0.0501 - val_accuracy: 0.9848\n",
            "Epoch 566/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9760 - val_loss: 0.0634 - val_accuracy: 0.9817\n",
            "Epoch 567/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9791 - val_loss: 0.0506 - val_accuracy: 0.9832\n",
            "Epoch 568/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0707 - accuracy: 0.9768 - val_loss: 0.0573 - val_accuracy: 0.9848\n",
            "Epoch 569/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9737 - val_loss: 0.0588 - val_accuracy: 0.9832\n",
            "Epoch 570/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0646 - accuracy: 0.9772 - val_loss: 0.0498 - val_accuracy: 0.9832\n",
            "Epoch 571/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9730 - val_loss: 0.0509 - val_accuracy: 0.9832\n",
            "Epoch 572/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0718 - accuracy: 0.9764 - val_loss: 0.0537 - val_accuracy: 0.9848\n",
            "Epoch 573/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0692 - accuracy: 0.9756 - val_loss: 0.0616 - val_accuracy: 0.9832\n",
            "Epoch 574/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0646 - accuracy: 0.9756 - val_loss: 0.0512 - val_accuracy: 0.9832\n",
            "Epoch 575/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9760 - val_loss: 0.0596 - val_accuracy: 0.9817\n",
            "Epoch 576/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9768 - val_loss: 0.0570 - val_accuracy: 0.9848\n",
            "Epoch 577/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0679 - accuracy: 0.9768 - val_loss: 0.0685 - val_accuracy: 0.9726\n",
            "Epoch 578/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0697 - accuracy: 0.9734 - val_loss: 0.0604 - val_accuracy: 0.9802\n",
            "Epoch 579/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9764 - val_loss: 0.0537 - val_accuracy: 0.9848\n",
            "Epoch 580/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9749 - val_loss: 0.0471 - val_accuracy: 0.9878\n",
            "Epoch 581/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0710 - accuracy: 0.9775 - val_loss: 0.0654 - val_accuracy: 0.9817\n",
            "Epoch 582/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9783 - val_loss: 0.0536 - val_accuracy: 0.9832\n",
            "Epoch 583/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9749 - val_loss: 0.1076 - val_accuracy: 0.9680\n",
            "Epoch 584/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0821 - accuracy: 0.9707 - val_loss: 0.0503 - val_accuracy: 0.9832\n",
            "Epoch 585/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0715 - accuracy: 0.9749 - val_loss: 0.0476 - val_accuracy: 0.9863\n",
            "Epoch 586/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.9749 - val_loss: 0.0585 - val_accuracy: 0.9848\n",
            "Epoch 587/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0679 - accuracy: 0.9779 - val_loss: 0.0680 - val_accuracy: 0.9787\n",
            "Epoch 588/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9768 - val_loss: 0.0507 - val_accuracy: 0.9848\n",
            "Epoch 589/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0749 - accuracy: 0.9741 - val_loss: 0.0673 - val_accuracy: 0.9802\n",
            "Epoch 590/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0700 - accuracy: 0.9756 - val_loss: 0.0508 - val_accuracy: 0.9832\n",
            "Epoch 591/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0699 - accuracy: 0.9756 - val_loss: 0.0677 - val_accuracy: 0.9832\n",
            "Epoch 592/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0790 - accuracy: 0.9760 - val_loss: 0.0597 - val_accuracy: 0.9832\n",
            "Epoch 593/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0766 - accuracy: 0.9737 - val_loss: 0.0535 - val_accuracy: 0.9863\n",
            "Epoch 594/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9745 - val_loss: 0.0508 - val_accuracy: 0.9832\n",
            "Epoch 595/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0773 - accuracy: 0.9741 - val_loss: 0.0482 - val_accuracy: 0.9848\n",
            "Epoch 596/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0762 - accuracy: 0.9760 - val_loss: 0.0492 - val_accuracy: 0.9848\n",
            "Epoch 597/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0749 - accuracy: 0.9737 - val_loss: 0.0552 - val_accuracy: 0.9848\n",
            "Epoch 598/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.9737 - val_loss: 0.0560 - val_accuracy: 0.9802\n",
            "Epoch 599/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9772 - val_loss: 0.0574 - val_accuracy: 0.9832\n",
            "Epoch 600/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0681 - accuracy: 0.9787 - val_loss: 0.0651 - val_accuracy: 0.9756\n",
            "Epoch 601/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0731 - accuracy: 0.9768 - val_loss: 0.0573 - val_accuracy: 0.9832\n",
            "Epoch 602/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0688 - accuracy: 0.9764 - val_loss: 0.0502 - val_accuracy: 0.9848\n",
            "Epoch 603/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.9745 - val_loss: 0.0594 - val_accuracy: 0.9817\n",
            "Epoch 604/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0651 - accuracy: 0.9783 - val_loss: 0.0494 - val_accuracy: 0.9863\n",
            "Epoch 605/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0694 - accuracy: 0.9791 - val_loss: 0.0501 - val_accuracy: 0.9848\n",
            "Epoch 606/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0682 - accuracy: 0.9795 - val_loss: 0.0639 - val_accuracy: 0.9817\n",
            "Epoch 607/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0655 - accuracy: 0.9775 - val_loss: 0.0537 - val_accuracy: 0.9848\n",
            "Epoch 608/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0715 - accuracy: 0.9726 - val_loss: 0.0485 - val_accuracy: 0.9848\n",
            "Epoch 609/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0716 - accuracy: 0.9745 - val_loss: 0.0695 - val_accuracy: 0.9787\n",
            "Epoch 610/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0685 - accuracy: 0.9756 - val_loss: 0.0520 - val_accuracy: 0.9863\n",
            "Epoch 611/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0695 - accuracy: 0.9802 - val_loss: 0.0521 - val_accuracy: 0.9848\n",
            "Epoch 612/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9802 - val_loss: 0.0487 - val_accuracy: 0.9848\n",
            "Epoch 613/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9772 - val_loss: 0.0474 - val_accuracy: 0.9863\n",
            "Epoch 614/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0678 - accuracy: 0.9783 - val_loss: 0.0569 - val_accuracy: 0.9848\n",
            "Epoch 615/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9772 - val_loss: 0.0495 - val_accuracy: 0.9863\n",
            "Epoch 616/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9756 - val_loss: 0.0541 - val_accuracy: 0.9832\n",
            "Epoch 617/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9745 - val_loss: 0.0879 - val_accuracy: 0.9680\n",
            "Epoch 618/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0662 - accuracy: 0.9760 - val_loss: 0.0529 - val_accuracy: 0.9848\n",
            "Epoch 619/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9760 - val_loss: 0.0657 - val_accuracy: 0.9832\n",
            "Epoch 620/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 0.9756 - val_loss: 0.0468 - val_accuracy: 0.9878\n",
            "Epoch 621/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0688 - accuracy: 0.9756 - val_loss: 0.0540 - val_accuracy: 0.9817\n",
            "Epoch 622/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9791 - val_loss: 0.0473 - val_accuracy: 0.9848\n",
            "Epoch 623/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9779 - val_loss: 0.0677 - val_accuracy: 0.9771\n",
            "Epoch 624/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0700 - accuracy: 0.9764 - val_loss: 0.0656 - val_accuracy: 0.9741\n",
            "Epoch 625/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9734 - val_loss: 0.0673 - val_accuracy: 0.9817\n",
            "Epoch 626/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0654 - accuracy: 0.9787 - val_loss: 0.0676 - val_accuracy: 0.9756\n",
            "Epoch 627/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0718 - accuracy: 0.9745 - val_loss: 0.0479 - val_accuracy: 0.9863\n",
            "Epoch 628/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9787 - val_loss: 0.0486 - val_accuracy: 0.9863\n",
            "Epoch 629/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9760 - val_loss: 0.0694 - val_accuracy: 0.9726\n",
            "Epoch 630/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0663 - accuracy: 0.9775 - val_loss: 0.0620 - val_accuracy: 0.9817\n",
            "Epoch 631/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0636 - accuracy: 0.9806 - val_loss: 0.0539 - val_accuracy: 0.9832\n",
            "Epoch 632/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9806 - val_loss: 0.0626 - val_accuracy: 0.9771\n",
            "Epoch 633/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0636 - accuracy: 0.9753 - val_loss: 0.0468 - val_accuracy: 0.9848\n",
            "Epoch 634/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0722 - accuracy: 0.9764 - val_loss: 0.0463 - val_accuracy: 0.9832\n",
            "Epoch 635/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9737 - val_loss: 0.0487 - val_accuracy: 0.9832\n",
            "Epoch 636/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0788 - accuracy: 0.9707 - val_loss: 0.0522 - val_accuracy: 0.9832\n",
            "Epoch 637/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9745 - val_loss: 0.0786 - val_accuracy: 0.9695\n",
            "Epoch 638/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9730 - val_loss: 0.0543 - val_accuracy: 0.9832\n",
            "Epoch 639/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0665 - accuracy: 0.9775 - val_loss: 0.0497 - val_accuracy: 0.9817\n",
            "Epoch 640/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0689 - accuracy: 0.9791 - val_loss: 0.0573 - val_accuracy: 0.9817\n",
            "Epoch 641/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0681 - accuracy: 0.9764 - val_loss: 0.0608 - val_accuracy: 0.9817\n",
            "Epoch 642/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0623 - accuracy: 0.9783 - val_loss: 0.0546 - val_accuracy: 0.9832\n",
            "Epoch 643/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0694 - accuracy: 0.9764 - val_loss: 0.0523 - val_accuracy: 0.9848\n",
            "Epoch 644/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0730 - accuracy: 0.9779 - val_loss: 0.0550 - val_accuracy: 0.9848\n",
            "Epoch 645/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9756 - val_loss: 0.0968 - val_accuracy: 0.9665\n",
            "Epoch 646/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0725 - accuracy: 0.9779 - val_loss: 0.0550 - val_accuracy: 0.9832\n",
            "Epoch 647/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0681 - accuracy: 0.9775 - val_loss: 0.0508 - val_accuracy: 0.9832\n",
            "Epoch 648/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0617 - accuracy: 0.9791 - val_loss: 0.0586 - val_accuracy: 0.9817\n",
            "Epoch 649/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0641 - accuracy: 0.9787 - val_loss: 0.0825 - val_accuracy: 0.9649\n",
            "Epoch 650/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9753 - val_loss: 0.0607 - val_accuracy: 0.9848\n",
            "Epoch 651/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0693 - accuracy: 0.9745 - val_loss: 0.0558 - val_accuracy: 0.9848\n",
            "Epoch 652/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0628 - accuracy: 0.9772 - val_loss: 0.0523 - val_accuracy: 0.9817\n",
            "Epoch 653/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0624 - accuracy: 0.9756 - val_loss: 0.0554 - val_accuracy: 0.9817\n",
            "Epoch 654/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9764 - val_loss: 0.0723 - val_accuracy: 0.9695\n",
            "Epoch 655/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0672 - accuracy: 0.9749 - val_loss: 0.0549 - val_accuracy: 0.9832\n",
            "Epoch 656/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9772 - val_loss: 0.0562 - val_accuracy: 0.9817\n",
            "Epoch 657/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0616 - accuracy: 0.9775 - val_loss: 0.0547 - val_accuracy: 0.9848\n",
            "Epoch 658/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0724 - accuracy: 0.9734 - val_loss: 0.0559 - val_accuracy: 0.9848\n",
            "Epoch 659/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.0561 - val_accuracy: 0.9832\n",
            "Epoch 660/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9795 - val_loss: 0.0508 - val_accuracy: 0.9832\n",
            "Epoch 661/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0603 - accuracy: 0.9817 - val_loss: 0.0686 - val_accuracy: 0.9741\n",
            "Epoch 662/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 0.9779 - val_loss: 0.0606 - val_accuracy: 0.9848\n",
            "Epoch 663/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0631 - accuracy: 0.9787 - val_loss: 0.0837 - val_accuracy: 0.9695\n",
            "Epoch 664/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9802 - val_loss: 0.0729 - val_accuracy: 0.9726\n",
            "Epoch 665/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0616 - accuracy: 0.9817 - val_loss: 0.0538 - val_accuracy: 0.9848\n",
            "Epoch 666/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0678 - accuracy: 0.9760 - val_loss: 0.0697 - val_accuracy: 0.9787\n",
            "Epoch 667/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0689 - accuracy: 0.9764 - val_loss: 0.0617 - val_accuracy: 0.9817\n",
            "Epoch 668/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0662 - accuracy: 0.9779 - val_loss: 0.0566 - val_accuracy: 0.9832\n",
            "Epoch 669/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0607 - accuracy: 0.9795 - val_loss: 0.0664 - val_accuracy: 0.9802\n",
            "Epoch 670/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.9764 - val_loss: 0.0593 - val_accuracy: 0.9771\n",
            "Epoch 671/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0873 - accuracy: 0.9718 - val_loss: 0.0689 - val_accuracy: 0.9741\n",
            "Epoch 672/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0751 - accuracy: 0.9737 - val_loss: 0.0516 - val_accuracy: 0.9832\n",
            "Epoch 673/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0794 - accuracy: 0.9711 - val_loss: 0.0552 - val_accuracy: 0.9832\n",
            "Epoch 674/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0713 - accuracy: 0.9779 - val_loss: 0.0977 - val_accuracy: 0.9680\n",
            "Epoch 675/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9768 - val_loss: 0.0903 - val_accuracy: 0.9695\n",
            "Epoch 676/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0853 - accuracy: 0.9711 - val_loss: 0.0763 - val_accuracy: 0.9756\n",
            "Epoch 677/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0870 - accuracy: 0.9737 - val_loss: 0.0652 - val_accuracy: 0.9756\n",
            "Epoch 678/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9745 - val_loss: 0.0640 - val_accuracy: 0.9817\n",
            "Epoch 679/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0733 - accuracy: 0.9741 - val_loss: 0.0673 - val_accuracy: 0.9756\n",
            "Epoch 680/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9787 - val_loss: 0.1011 - val_accuracy: 0.9634\n",
            "Epoch 681/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.9749 - val_loss: 0.0588 - val_accuracy: 0.9832\n",
            "Epoch 682/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0679 - accuracy: 0.9753 - val_loss: 0.0494 - val_accuracy: 0.9878\n",
            "Epoch 683/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9787 - val_loss: 0.0717 - val_accuracy: 0.9756\n",
            "Epoch 684/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0695 - accuracy: 0.9783 - val_loss: 0.0523 - val_accuracy: 0.9832\n",
            "Epoch 685/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0618 - accuracy: 0.9802 - val_loss: 0.0485 - val_accuracy: 0.9832\n",
            "Epoch 686/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 0.9768 - val_loss: 0.0661 - val_accuracy: 0.9817\n",
            "Epoch 687/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0676 - accuracy: 0.9768 - val_loss: 0.0524 - val_accuracy: 0.9848\n",
            "Epoch 688/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0656 - accuracy: 0.9775 - val_loss: 0.0522 - val_accuracy: 0.9863\n",
            "Epoch 689/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9760 - val_loss: 0.0488 - val_accuracy: 0.9832\n",
            "Epoch 690/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0652 - accuracy: 0.9768 - val_loss: 0.0822 - val_accuracy: 0.9710\n",
            "Epoch 691/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0698 - accuracy: 0.9760 - val_loss: 0.0621 - val_accuracy: 0.9817\n",
            "Epoch 692/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0672 - accuracy: 0.9768 - val_loss: 0.0492 - val_accuracy: 0.9878\n",
            "Epoch 693/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0765 - accuracy: 0.9730 - val_loss: 0.0495 - val_accuracy: 0.9878\n",
            "Epoch 694/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0677 - accuracy: 0.9772 - val_loss: 0.0586 - val_accuracy: 0.9802\n",
            "Epoch 695/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9795 - val_loss: 0.0664 - val_accuracy: 0.9771\n",
            "Epoch 696/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.9749 - val_loss: 0.0854 - val_accuracy: 0.9710\n",
            "Epoch 697/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0629 - accuracy: 0.9798 - val_loss: 0.0668 - val_accuracy: 0.9817\n",
            "Epoch 698/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0636 - accuracy: 0.9779 - val_loss: 0.0491 - val_accuracy: 0.9878\n",
            "Epoch 699/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0726 - accuracy: 0.9753 - val_loss: 0.0533 - val_accuracy: 0.9832\n",
            "Epoch 700/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0693 - accuracy: 0.9764 - val_loss: 0.0526 - val_accuracy: 0.9848\n",
            "Epoch 701/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0626 - accuracy: 0.9802 - val_loss: 0.0548 - val_accuracy: 0.9832\n",
            "Epoch 702/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0681 - accuracy: 0.9772 - val_loss: 0.0487 - val_accuracy: 0.9848\n",
            "Epoch 703/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9730 - val_loss: 0.0494 - val_accuracy: 0.9863\n",
            "Epoch 704/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9760 - val_loss: 0.0493 - val_accuracy: 0.9817\n",
            "Epoch 705/1000\n",
            "83/83 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9772 - val_loss: 0.0669 - val_accuracy: 0.9771\n",
            "Epoch 706/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0692 - accuracy: 0.9764 - val_loss: 0.0599 - val_accuracy: 0.9832\n",
            "Epoch 707/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0613 - accuracy: 0.9798 - val_loss: 0.0720 - val_accuracy: 0.9741\n",
            "Epoch 708/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0737 - accuracy: 0.9749 - val_loss: 0.0505 - val_accuracy: 0.9832\n",
            "Epoch 709/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0658 - accuracy: 0.9779 - val_loss: 0.0514 - val_accuracy: 0.9848\n",
            "Epoch 710/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 0.9756 - val_loss: 0.0568 - val_accuracy: 0.9802\n",
            "Epoch 711/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0616 - accuracy: 0.9795 - val_loss: 0.0546 - val_accuracy: 0.9832\n",
            "Epoch 712/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0658 - accuracy: 0.9768 - val_loss: 0.0578 - val_accuracy: 0.9848\n",
            "Epoch 713/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0585 - accuracy: 0.9806 - val_loss: 0.0493 - val_accuracy: 0.9832\n",
            "Epoch 714/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0653 - accuracy: 0.9753 - val_loss: 0.0555 - val_accuracy: 0.9848\n",
            "Epoch 715/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0645 - accuracy: 0.9791 - val_loss: 0.0470 - val_accuracy: 0.9863\n",
            "Epoch 716/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0600 - accuracy: 0.9806 - val_loss: 0.0542 - val_accuracy: 0.9832\n",
            "Epoch 717/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0647 - accuracy: 0.9798 - val_loss: 0.0533 - val_accuracy: 0.9848\n",
            "Epoch 718/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0655 - accuracy: 0.9783 - val_loss: 0.0734 - val_accuracy: 0.9771\n",
            "Epoch 719/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0770 - accuracy: 0.9749 - val_loss: 0.0702 - val_accuracy: 0.9787\n",
            "Epoch 720/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0800 - accuracy: 0.9734 - val_loss: 0.0597 - val_accuracy: 0.9802\n",
            "Epoch 721/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0914 - accuracy: 0.9646 - val_loss: 0.0523 - val_accuracy: 0.9817\n",
            "Epoch 722/1000\n",
            "83/83 [==============================] - 1s 7ms/step - loss: 0.0758 - accuracy: 0.9726 - val_loss: 0.0620 - val_accuracy: 0.9832\n",
            "Epoch 723/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.9764 - val_loss: 0.0582 - val_accuracy: 0.9832\n",
            "Epoch 724/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0719 - accuracy: 0.9760 - val_loss: 0.0999 - val_accuracy: 0.9649\n",
            "Epoch 725/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0692 - accuracy: 0.9730 - val_loss: 0.0580 - val_accuracy: 0.9848\n",
            "Epoch 726/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0715 - accuracy: 0.9753 - val_loss: 0.0484 - val_accuracy: 0.9878\n",
            "Epoch 727/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0663 - accuracy: 0.9787 - val_loss: 0.0552 - val_accuracy: 0.9832\n",
            "Epoch 728/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0660 - accuracy: 0.9779 - val_loss: 0.0604 - val_accuracy: 0.9802\n",
            "Epoch 729/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9772 - val_loss: 0.0546 - val_accuracy: 0.9817\n",
            "Epoch 730/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0642 - accuracy: 0.9772 - val_loss: 0.0501 - val_accuracy: 0.9832\n",
            "Epoch 731/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0725 - accuracy: 0.9756 - val_loss: 0.0592 - val_accuracy: 0.9832\n",
            "Epoch 732/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0656 - accuracy: 0.9764 - val_loss: 0.0505 - val_accuracy: 0.9878\n",
            "Epoch 733/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0688 - accuracy: 0.9791 - val_loss: 0.0629 - val_accuracy: 0.9832\n",
            "Epoch 734/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0640 - accuracy: 0.9787 - val_loss: 0.0500 - val_accuracy: 0.9832\n",
            "Epoch 735/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9764 - val_loss: 0.0538 - val_accuracy: 0.9848\n",
            "Epoch 736/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0854 - accuracy: 0.9677 - val_loss: 0.0534 - val_accuracy: 0.9848\n",
            "Epoch 737/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0658 - accuracy: 0.9749 - val_loss: 0.0747 - val_accuracy: 0.9741\n",
            "Epoch 738/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0659 - accuracy: 0.9737 - val_loss: 0.0637 - val_accuracy: 0.9817\n",
            "Epoch 739/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0671 - accuracy: 0.9741 - val_loss: 0.0511 - val_accuracy: 0.9848\n",
            "Epoch 740/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0678 - accuracy: 0.9756 - val_loss: 0.0517 - val_accuracy: 0.9817\n",
            "Epoch 741/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0704 - accuracy: 0.9745 - val_loss: 0.0518 - val_accuracy: 0.9848\n",
            "Epoch 742/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0810 - accuracy: 0.9711 - val_loss: 0.0860 - val_accuracy: 0.9695\n",
            "Epoch 743/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0688 - accuracy: 0.9753 - val_loss: 0.0668 - val_accuracy: 0.9817\n",
            "Epoch 744/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0613 - accuracy: 0.9768 - val_loss: 0.0528 - val_accuracy: 0.9832\n",
            "Epoch 745/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0648 - accuracy: 0.9768 - val_loss: 0.0587 - val_accuracy: 0.9832\n",
            "Epoch 746/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0593 - accuracy: 0.9783 - val_loss: 0.0642 - val_accuracy: 0.9817\n",
            "Epoch 747/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0668 - accuracy: 0.9745 - val_loss: 0.0574 - val_accuracy: 0.9832\n",
            "Epoch 748/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9775 - val_loss: 0.0572 - val_accuracy: 0.9817\n",
            "Epoch 749/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0616 - accuracy: 0.9787 - val_loss: 0.0520 - val_accuracy: 0.9848\n",
            "Epoch 750/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0607 - accuracy: 0.9798 - val_loss: 0.0563 - val_accuracy: 0.9832\n",
            "Epoch 751/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0601 - accuracy: 0.9795 - val_loss: 0.0601 - val_accuracy: 0.9802\n",
            "Epoch 752/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0644 - accuracy: 0.9768 - val_loss: 0.0521 - val_accuracy: 0.9832\n",
            "Epoch 753/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0646 - accuracy: 0.9772 - val_loss: 0.0618 - val_accuracy: 0.9817\n",
            "Epoch 754/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0630 - accuracy: 0.9783 - val_loss: 0.0609 - val_accuracy: 0.9832\n",
            "Epoch 755/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0618 - accuracy: 0.9783 - val_loss: 0.0476 - val_accuracy: 0.9863\n",
            "Epoch 756/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0624 - accuracy: 0.9737 - val_loss: 0.0481 - val_accuracy: 0.9848\n",
            "Epoch 757/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0671 - accuracy: 0.9768 - val_loss: 0.0637 - val_accuracy: 0.9817\n",
            "Epoch 758/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0588 - accuracy: 0.9795 - val_loss: 0.0532 - val_accuracy: 0.9832\n",
            "Epoch 759/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0556 - accuracy: 0.9798 - val_loss: 0.0662 - val_accuracy: 0.9802\n",
            "Epoch 760/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0577 - accuracy: 0.9798 - val_loss: 0.0578 - val_accuracy: 0.9832\n",
            "Epoch 761/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0598 - accuracy: 0.9806 - val_loss: 0.0738 - val_accuracy: 0.9756\n",
            "Epoch 762/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0622 - accuracy: 0.9802 - val_loss: 0.0566 - val_accuracy: 0.9848\n",
            "Epoch 763/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0625 - accuracy: 0.9760 - val_loss: 0.0636 - val_accuracy: 0.9817\n",
            "Epoch 764/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0606 - accuracy: 0.9779 - val_loss: 0.0575 - val_accuracy: 0.9817\n",
            "Epoch 765/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0576 - accuracy: 0.9821 - val_loss: 0.0513 - val_accuracy: 0.9863\n",
            "Epoch 766/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0560 - accuracy: 0.9852 - val_loss: 0.0708 - val_accuracy: 0.9787\n",
            "Epoch 767/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0607 - accuracy: 0.9795 - val_loss: 0.0887 - val_accuracy: 0.9665\n",
            "Epoch 768/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0605 - accuracy: 0.9806 - val_loss: 0.0564 - val_accuracy: 0.9817\n",
            "Epoch 769/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0599 - accuracy: 0.9817 - val_loss: 0.0699 - val_accuracy: 0.9802\n",
            "Epoch 770/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0602 - accuracy: 0.9783 - val_loss: 0.0494 - val_accuracy: 0.9848\n",
            "Epoch 771/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0609 - accuracy: 0.9775 - val_loss: 0.0529 - val_accuracy: 0.9848\n",
            "Epoch 772/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0592 - accuracy: 0.9814 - val_loss: 0.0584 - val_accuracy: 0.9817\n",
            "Epoch 773/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0593 - accuracy: 0.9787 - val_loss: 0.0628 - val_accuracy: 0.9802\n",
            "Epoch 774/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0597 - accuracy: 0.9768 - val_loss: 0.0611 - val_accuracy: 0.9802\n",
            "Epoch 775/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0528 - accuracy: 0.9840 - val_loss: 0.0593 - val_accuracy: 0.9817\n",
            "Epoch 776/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.9783 - val_loss: 0.0637 - val_accuracy: 0.9832\n",
            "Epoch 777/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9814 - val_loss: 0.0541 - val_accuracy: 0.9848\n",
            "Epoch 778/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.9806 - val_loss: 0.0506 - val_accuracy: 0.9863\n",
            "Epoch 779/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0578 - accuracy: 0.9798 - val_loss: 0.0614 - val_accuracy: 0.9832\n",
            "Epoch 780/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9814 - val_loss: 0.0526 - val_accuracy: 0.9848\n",
            "Epoch 781/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0556 - accuracy: 0.9814 - val_loss: 0.0514 - val_accuracy: 0.9848\n",
            "Epoch 782/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0622 - accuracy: 0.9768 - val_loss: 0.0670 - val_accuracy: 0.9832\n",
            "Epoch 783/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0553 - accuracy: 0.9821 - val_loss: 0.0643 - val_accuracy: 0.9787\n",
            "Epoch 784/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0615 - accuracy: 0.9775 - val_loss: 0.0590 - val_accuracy: 0.9832\n",
            "Epoch 785/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0576 - accuracy: 0.9806 - val_loss: 0.0500 - val_accuracy: 0.9863\n",
            "Epoch 786/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0547 - accuracy: 0.9817 - val_loss: 0.0559 - val_accuracy: 0.9848\n",
            "Epoch 787/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9806 - val_loss: 0.0510 - val_accuracy: 0.9848\n",
            "Epoch 788/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0557 - accuracy: 0.9787 - val_loss: 0.0607 - val_accuracy: 0.9817\n",
            "Epoch 789/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0673 - val_accuracy: 0.9787\n",
            "Epoch 790/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0583 - accuracy: 0.9795 - val_loss: 0.0550 - val_accuracy: 0.9863\n",
            "Epoch 791/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.9806 - val_loss: 0.0517 - val_accuracy: 0.9848\n",
            "Epoch 792/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0562 - accuracy: 0.9798 - val_loss: 0.0504 - val_accuracy: 0.9832\n",
            "Epoch 793/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0547 - accuracy: 0.9787 - val_loss: 0.0568 - val_accuracy: 0.9848\n",
            "Epoch 794/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0557 - accuracy: 0.9802 - val_loss: 0.0546 - val_accuracy: 0.9848\n",
            "Epoch 795/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0589 - accuracy: 0.9791 - val_loss: 0.0619 - val_accuracy: 0.9802\n",
            "Epoch 796/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0604 - accuracy: 0.9783 - val_loss: 0.0492 - val_accuracy: 0.9863\n",
            "Epoch 797/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.9810 - val_loss: 0.0522 - val_accuracy: 0.9832\n",
            "Epoch 798/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0561 - accuracy: 0.9810 - val_loss: 0.0584 - val_accuracy: 0.9817\n",
            "Epoch 799/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9798 - val_loss: 0.0665 - val_accuracy: 0.9771\n",
            "Epoch 800/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0574 - accuracy: 0.9817 - val_loss: 0.0593 - val_accuracy: 0.9802\n",
            "Epoch 801/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.0771 - val_accuracy: 0.9710\n",
            "Epoch 802/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0597 - accuracy: 0.9802 - val_loss: 0.0636 - val_accuracy: 0.9802\n",
            "Epoch 803/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 0.9791 - val_loss: 0.0629 - val_accuracy: 0.9817\n",
            "Epoch 804/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9806 - val_loss: 0.0588 - val_accuracy: 0.9817\n",
            "Epoch 805/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0530 - accuracy: 0.9829 - val_loss: 0.0549 - val_accuracy: 0.9848\n",
            "Epoch 806/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.9802 - val_loss: 0.0506 - val_accuracy: 0.9848\n",
            "Epoch 807/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9810 - val_loss: 0.0572 - val_accuracy: 0.9832\n",
            "Epoch 808/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0554 - accuracy: 0.9798 - val_loss: 0.0741 - val_accuracy: 0.9741\n",
            "Epoch 809/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 0.9825 - val_loss: 0.0558 - val_accuracy: 0.9832\n",
            "Epoch 810/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0564 - accuracy: 0.9795 - val_loss: 0.0654 - val_accuracy: 0.9817\n",
            "Epoch 811/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0514 - accuracy: 0.9817 - val_loss: 0.0663 - val_accuracy: 0.9802\n",
            "Epoch 812/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0593 - accuracy: 0.9775 - val_loss: 0.0523 - val_accuracy: 0.9878\n",
            "Epoch 813/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.9783 - val_loss: 0.0648 - val_accuracy: 0.9802\n",
            "Epoch 814/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0567 - accuracy: 0.9787 - val_loss: 0.0781 - val_accuracy: 0.9710\n",
            "Epoch 815/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0562 - accuracy: 0.9817 - val_loss: 0.0587 - val_accuracy: 0.9817\n",
            "Epoch 816/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9791 - val_loss: 0.0563 - val_accuracy: 0.9817\n",
            "Epoch 817/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9821 - val_loss: 0.0557 - val_accuracy: 0.9848\n",
            "Epoch 818/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0529 - accuracy: 0.9836 - val_loss: 0.0678 - val_accuracy: 0.9787\n",
            "Epoch 819/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0586 - accuracy: 0.9795 - val_loss: 0.0691 - val_accuracy: 0.9817\n",
            "Epoch 820/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0587 - accuracy: 0.9787 - val_loss: 0.0551 - val_accuracy: 0.9848\n",
            "Epoch 821/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0512 - accuracy: 0.9825 - val_loss: 0.1372 - val_accuracy: 0.9527\n",
            "Epoch 822/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0901 - accuracy: 0.9688 - val_loss: 0.0548 - val_accuracy: 0.9832\n",
            "Epoch 823/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0646 - accuracy: 0.9772 - val_loss: 0.0492 - val_accuracy: 0.9832\n",
            "Epoch 824/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0610 - val_accuracy: 0.9817\n",
            "Epoch 825/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0546 - val_accuracy: 0.9848\n",
            "Epoch 826/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0582 - accuracy: 0.9817 - val_loss: 0.0604 - val_accuracy: 0.9817\n",
            "Epoch 827/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0606 - accuracy: 0.9783 - val_loss: 0.0579 - val_accuracy: 0.9848\n",
            "Epoch 828/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0584 - accuracy: 0.9787 - val_loss: 0.0803 - val_accuracy: 0.9726\n",
            "Epoch 829/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0514 - accuracy: 0.9791 - val_loss: 0.0625 - val_accuracy: 0.9802\n",
            "Epoch 830/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 0.9821 - val_loss: 0.0841 - val_accuracy: 0.9695\n",
            "Epoch 831/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0547 - accuracy: 0.9795 - val_loss: 0.0716 - val_accuracy: 0.9802\n",
            "Epoch 832/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0577 - accuracy: 0.9798 - val_loss: 0.0639 - val_accuracy: 0.9802\n",
            "Epoch 833/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0564 - accuracy: 0.9825 - val_loss: 0.0501 - val_accuracy: 0.9848\n",
            "Epoch 834/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0543 - accuracy: 0.9833 - val_loss: 0.0544 - val_accuracy: 0.9848\n",
            "Epoch 835/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0541 - accuracy: 0.9806 - val_loss: 0.0537 - val_accuracy: 0.9848\n",
            "Epoch 836/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 0.9795 - val_loss: 0.0479 - val_accuracy: 0.9817\n",
            "Epoch 837/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0639 - accuracy: 0.9802 - val_loss: 0.0743 - val_accuracy: 0.9741\n",
            "Epoch 838/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0634 - accuracy: 0.9745 - val_loss: 0.0719 - val_accuracy: 0.9787\n",
            "Epoch 839/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0518 - val_accuracy: 0.9863\n",
            "Epoch 840/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0537 - accuracy: 0.9817 - val_loss: 0.0605 - val_accuracy: 0.9817\n",
            "Epoch 841/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0534 - accuracy: 0.9821 - val_loss: 0.0762 - val_accuracy: 0.9726\n",
            "Epoch 842/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0529 - accuracy: 0.9829 - val_loss: 0.0700 - val_accuracy: 0.9802\n",
            "Epoch 843/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0556 - accuracy: 0.9814 - val_loss: 0.0573 - val_accuracy: 0.9802\n",
            "Epoch 844/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0576 - accuracy: 0.9814 - val_loss: 0.0649 - val_accuracy: 0.9802\n",
            "Epoch 845/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0556 - accuracy: 0.9795 - val_loss: 0.0646 - val_accuracy: 0.9817\n",
            "Epoch 846/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0551 - accuracy: 0.9817 - val_loss: 0.0499 - val_accuracy: 0.9863\n",
            "Epoch 847/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0506 - accuracy: 0.9848 - val_loss: 0.0729 - val_accuracy: 0.9741\n",
            "Epoch 848/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0575 - accuracy: 0.9768 - val_loss: 0.0605 - val_accuracy: 0.9802\n",
            "Epoch 849/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9825 - val_loss: 0.0648 - val_accuracy: 0.9802\n",
            "Epoch 850/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0536 - accuracy: 0.9817 - val_loss: 0.0583 - val_accuracy: 0.9817\n",
            "Epoch 851/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0549 - accuracy: 0.9795 - val_loss: 0.1007 - val_accuracy: 0.9573\n",
            "Epoch 852/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0602 - accuracy: 0.9802 - val_loss: 0.0586 - val_accuracy: 0.9848\n",
            "Epoch 853/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0598 - accuracy: 0.9795 - val_loss: 0.0537 - val_accuracy: 0.9863\n",
            "Epoch 854/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 0.9825 - val_loss: 0.0501 - val_accuracy: 0.9878\n",
            "Epoch 855/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.9795 - val_loss: 0.0840 - val_accuracy: 0.9665\n",
            "Epoch 856/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0543 - accuracy: 0.9798 - val_loss: 0.0508 - val_accuracy: 0.9848\n",
            "Epoch 857/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 0.9821 - val_loss: 0.0703 - val_accuracy: 0.9802\n",
            "Epoch 858/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0536 - accuracy: 0.9798 - val_loss: 0.0743 - val_accuracy: 0.9787\n",
            "Epoch 859/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0549 - accuracy: 0.9806 - val_loss: 0.0541 - val_accuracy: 0.9848\n",
            "Epoch 860/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0541 - accuracy: 0.9798 - val_loss: 0.0573 - val_accuracy: 0.9832\n",
            "Epoch 861/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0499 - accuracy: 0.9821 - val_loss: 0.0541 - val_accuracy: 0.9832\n",
            "Epoch 862/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0552 - accuracy: 0.9798 - val_loss: 0.0552 - val_accuracy: 0.9832\n",
            "Epoch 863/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0523 - accuracy: 0.9825 - val_loss: 0.0502 - val_accuracy: 0.9848\n",
            "Epoch 864/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0546 - accuracy: 0.9814 - val_loss: 0.0486 - val_accuracy: 0.9863\n",
            "Epoch 865/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0512 - accuracy: 0.9817 - val_loss: 0.0722 - val_accuracy: 0.9756\n",
            "Epoch 866/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0490 - accuracy: 0.9817 - val_loss: 0.0570 - val_accuracy: 0.9848\n",
            "Epoch 867/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0554 - accuracy: 0.9814 - val_loss: 0.0576 - val_accuracy: 0.9848\n",
            "Epoch 868/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0535 - accuracy: 0.9814 - val_loss: 0.0534 - val_accuracy: 0.9848\n",
            "Epoch 869/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 0.9821 - val_loss: 0.0519 - val_accuracy: 0.9848\n",
            "Epoch 870/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0498 - accuracy: 0.9791 - val_loss: 0.0567 - val_accuracy: 0.9832\n",
            "Epoch 871/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9814 - val_loss: 0.0905 - val_accuracy: 0.9634\n",
            "Epoch 872/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0520 - accuracy: 0.9825 - val_loss: 0.0772 - val_accuracy: 0.9756\n",
            "Epoch 873/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0557 - accuracy: 0.9787 - val_loss: 0.0869 - val_accuracy: 0.9649\n",
            "Epoch 874/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0555 - accuracy: 0.9791 - val_loss: 0.0505 - val_accuracy: 0.9863\n",
            "Epoch 875/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 0.9798 - val_loss: 0.0637 - val_accuracy: 0.9787\n",
            "Epoch 876/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0596 - accuracy: 0.9802 - val_loss: 0.0529 - val_accuracy: 0.9832\n",
            "Epoch 877/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0485 - accuracy: 0.9848 - val_loss: 0.0693 - val_accuracy: 0.9771\n",
            "Epoch 878/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.9848 - val_loss: 0.0533 - val_accuracy: 0.9863\n",
            "Epoch 879/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0557 - accuracy: 0.9810 - val_loss: 0.0608 - val_accuracy: 0.9802\n",
            "Epoch 880/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0504 - accuracy: 0.9810 - val_loss: 0.0559 - val_accuracy: 0.9848\n",
            "Epoch 881/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0476 - accuracy: 0.9848 - val_loss: 0.0518 - val_accuracy: 0.9863\n",
            "Epoch 882/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0549 - accuracy: 0.9787 - val_loss: 0.0741 - val_accuracy: 0.9726\n",
            "Epoch 883/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0499 - accuracy: 0.9829 - val_loss: 0.0830 - val_accuracy: 0.9695\n",
            "Epoch 884/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 0.9779 - val_loss: 0.0762 - val_accuracy: 0.9726\n",
            "Epoch 885/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0518 - accuracy: 0.9844 - val_loss: 0.0674 - val_accuracy: 0.9787\n",
            "Epoch 886/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9840 - val_loss: 0.0714 - val_accuracy: 0.9787\n",
            "Epoch 887/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0556 - accuracy: 0.9802 - val_loss: 0.0492 - val_accuracy: 0.9863\n",
            "Epoch 888/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9833 - val_loss: 0.0640 - val_accuracy: 0.9817\n",
            "Epoch 889/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0592 - accuracy: 0.9798 - val_loss: 0.0782 - val_accuracy: 0.9695\n",
            "Epoch 890/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0512 - accuracy: 0.9814 - val_loss: 0.0552 - val_accuracy: 0.9832\n",
            "Epoch 891/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0509 - accuracy: 0.9817 - val_loss: 0.0569 - val_accuracy: 0.9832\n",
            "Epoch 892/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0522 - accuracy: 0.9814 - val_loss: 0.0589 - val_accuracy: 0.9817\n",
            "Epoch 893/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9817 - val_loss: 0.0584 - val_accuracy: 0.9817\n",
            "Epoch 894/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0550 - accuracy: 0.9795 - val_loss: 0.0551 - val_accuracy: 0.9848\n",
            "Epoch 895/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0530 - accuracy: 0.9802 - val_loss: 0.0747 - val_accuracy: 0.9741\n",
            "Epoch 896/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0545 - accuracy: 0.9814 - val_loss: 0.0648 - val_accuracy: 0.9771\n",
            "Epoch 897/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0507 - accuracy: 0.9810 - val_loss: 0.0612 - val_accuracy: 0.9832\n",
            "Epoch 898/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0532 - accuracy: 0.9814 - val_loss: 0.0706 - val_accuracy: 0.9802\n",
            "Epoch 899/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0532 - accuracy: 0.9795 - val_loss: 0.0546 - val_accuracy: 0.9832\n",
            "Epoch 900/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0515 - accuracy: 0.9814 - val_loss: 0.0486 - val_accuracy: 0.9863\n",
            "Epoch 901/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0751 - accuracy: 0.9741 - val_loss: 0.0503 - val_accuracy: 0.9817\n",
            "Epoch 902/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0711 - accuracy: 0.9753 - val_loss: 0.0556 - val_accuracy: 0.9802\n",
            "Epoch 903/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0669 - val_accuracy: 0.9756\n",
            "Epoch 904/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0614 - accuracy: 0.9783 - val_loss: 0.0497 - val_accuracy: 0.9817\n",
            "Epoch 905/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0626 - accuracy: 0.9772 - val_loss: 0.0601 - val_accuracy: 0.9802\n",
            "Epoch 906/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0662 - accuracy: 0.9745 - val_loss: 0.0611 - val_accuracy: 0.9787\n",
            "Epoch 907/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0611 - accuracy: 0.9775 - val_loss: 0.0760 - val_accuracy: 0.9726\n",
            "Epoch 908/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0600 - accuracy: 0.9798 - val_loss: 0.0690 - val_accuracy: 0.9787\n",
            "Epoch 909/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0589 - accuracy: 0.9795 - val_loss: 0.0632 - val_accuracy: 0.9817\n",
            "Epoch 910/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0583 - accuracy: 0.9795 - val_loss: 0.1002 - val_accuracy: 0.9634\n",
            "Epoch 911/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0851 - accuracy: 0.9741 - val_loss: 0.0497 - val_accuracy: 0.9863\n",
            "Epoch 912/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9779 - val_loss: 0.0537 - val_accuracy: 0.9878\n",
            "Epoch 913/1000\n",
            "83/83 [==============================] - 1s 7ms/step - loss: 0.0657 - accuracy: 0.9760 - val_loss: 0.0625 - val_accuracy: 0.9802\n",
            "Epoch 914/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0643 - accuracy: 0.9783 - val_loss: 0.0562 - val_accuracy: 0.9817\n",
            "Epoch 915/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0607 - accuracy: 0.9802 - val_loss: 0.0527 - val_accuracy: 0.9817\n",
            "Epoch 916/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0652 - accuracy: 0.9760 - val_loss: 0.0505 - val_accuracy: 0.9832\n",
            "Epoch 917/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0637 - accuracy: 0.9791 - val_loss: 0.0724 - val_accuracy: 0.9771\n",
            "Epoch 918/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0581 - accuracy: 0.9798 - val_loss: 0.0496 - val_accuracy: 0.9848\n",
            "Epoch 919/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0605 - accuracy: 0.9787 - val_loss: 0.0683 - val_accuracy: 0.9756\n",
            "Epoch 920/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.9798 - val_loss: 0.0519 - val_accuracy: 0.9848\n",
            "Epoch 921/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0772 - accuracy: 0.9722 - val_loss: 0.0514 - val_accuracy: 0.9832\n",
            "Epoch 922/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0703 - accuracy: 0.9764 - val_loss: 0.0513 - val_accuracy: 0.9817\n",
            "Epoch 923/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0669 - accuracy: 0.9756 - val_loss: 0.0535 - val_accuracy: 0.9832\n",
            "Epoch 924/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0621 - accuracy: 0.9779 - val_loss: 0.0585 - val_accuracy: 0.9787\n",
            "Epoch 925/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0638 - accuracy: 0.9768 - val_loss: 0.0595 - val_accuracy: 0.9802\n",
            "Epoch 926/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0668 - accuracy: 0.9756 - val_loss: 0.0553 - val_accuracy: 0.9817\n",
            "Epoch 927/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0604 - accuracy: 0.9787 - val_loss: 0.0476 - val_accuracy: 0.9832\n",
            "Epoch 928/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0649 - accuracy: 0.9768 - val_loss: 0.0566 - val_accuracy: 0.9832\n",
            "Epoch 929/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0616 - accuracy: 0.9802 - val_loss: 0.0598 - val_accuracy: 0.9817\n",
            "Epoch 930/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0585 - accuracy: 0.9772 - val_loss: 0.0621 - val_accuracy: 0.9817\n",
            "Epoch 931/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0556 - accuracy: 0.9791 - val_loss: 0.0466 - val_accuracy: 0.9848\n",
            "Epoch 932/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0542 - accuracy: 0.9821 - val_loss: 0.0620 - val_accuracy: 0.9817\n",
            "Epoch 933/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0590 - accuracy: 0.9806 - val_loss: 0.0494 - val_accuracy: 0.9817\n",
            "Epoch 934/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 0.9806 - val_loss: 0.0699 - val_accuracy: 0.9756\n",
            "Epoch 935/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0588 - accuracy: 0.9775 - val_loss: 0.0556 - val_accuracy: 0.9848\n",
            "Epoch 936/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0621 - accuracy: 0.9779 - val_loss: 0.0686 - val_accuracy: 0.9802\n",
            "Epoch 937/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0537 - accuracy: 0.9814 - val_loss: 0.0487 - val_accuracy: 0.9863\n",
            "Epoch 938/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0530 - accuracy: 0.9829 - val_loss: 0.0581 - val_accuracy: 0.9832\n",
            "Epoch 939/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0574 - accuracy: 0.9795 - val_loss: 0.0590 - val_accuracy: 0.9787\n",
            "Epoch 940/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0558 - accuracy: 0.9779 - val_loss: 0.0655 - val_accuracy: 0.9817\n",
            "Epoch 941/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0568 - accuracy: 0.9798 - val_loss: 0.0547 - val_accuracy: 0.9863\n",
            "Epoch 942/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0552 - accuracy: 0.9814 - val_loss: 0.0754 - val_accuracy: 0.9787\n",
            "Epoch 943/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 0.9821 - val_loss: 0.0578 - val_accuracy: 0.9832\n",
            "Epoch 944/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0535 - accuracy: 0.9798 - val_loss: 0.0693 - val_accuracy: 0.9787\n",
            "Epoch 945/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0544 - accuracy: 0.9810 - val_loss: 0.0670 - val_accuracy: 0.9802\n",
            "Epoch 946/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0560 - accuracy: 0.9795 - val_loss: 0.0628 - val_accuracy: 0.9787\n",
            "Epoch 947/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0553 - accuracy: 0.9802 - val_loss: 0.0546 - val_accuracy: 0.9848\n",
            "Epoch 948/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0492 - accuracy: 0.9810 - val_loss: 0.0484 - val_accuracy: 0.9848\n",
            "Epoch 949/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.9791 - val_loss: 0.0515 - val_accuracy: 0.9848\n",
            "Epoch 950/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0572 - accuracy: 0.9787 - val_loss: 0.0961 - val_accuracy: 0.9634\n",
            "Epoch 951/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0562 - accuracy: 0.9787 - val_loss: 0.0558 - val_accuracy: 0.9817\n",
            "Epoch 952/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0547 - accuracy: 0.9833 - val_loss: 0.0683 - val_accuracy: 0.9771\n",
            "Epoch 953/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0524 - accuracy: 0.9821 - val_loss: 0.0564 - val_accuracy: 0.9832\n",
            "Epoch 954/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9840 - val_loss: 0.0559 - val_accuracy: 0.9863\n",
            "Epoch 955/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0524 - accuracy: 0.9817 - val_loss: 0.0731 - val_accuracy: 0.9787\n",
            "Epoch 956/1000\n",
            "83/83 [==============================] - 1s 7ms/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0477 - val_accuracy: 0.9848\n",
            "Epoch 957/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0546 - accuracy: 0.9829 - val_loss: 0.0531 - val_accuracy: 0.9863\n",
            "Epoch 958/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0534 - accuracy: 0.9844 - val_loss: 0.0613 - val_accuracy: 0.9817\n",
            "Epoch 959/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0551 - accuracy: 0.9825 - val_loss: 0.0473 - val_accuracy: 0.9863\n",
            "Epoch 960/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0523 - accuracy: 0.9798 - val_loss: 0.0569 - val_accuracy: 0.9832\n",
            "Epoch 961/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0569 - accuracy: 0.9798 - val_loss: 0.0545 - val_accuracy: 0.9832\n",
            "Epoch 962/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0530 - accuracy: 0.9821 - val_loss: 0.0559 - val_accuracy: 0.9832\n",
            "Epoch 963/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.0543 - val_accuracy: 0.9848\n",
            "Epoch 964/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0558 - accuracy: 0.9787 - val_loss: 0.0645 - val_accuracy: 0.9817\n",
            "Epoch 965/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0538 - accuracy: 0.9806 - val_loss: 0.0646 - val_accuracy: 0.9802\n",
            "Epoch 966/1000\n",
            "83/83 [==============================] - 1s 7ms/step - loss: 0.0510 - accuracy: 0.9795 - val_loss: 0.0657 - val_accuracy: 0.9802\n",
            "Epoch 967/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0513 - accuracy: 0.9840 - val_loss: 0.0606 - val_accuracy: 0.9802\n",
            "Epoch 968/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0547 - accuracy: 0.9814 - val_loss: 0.0560 - val_accuracy: 0.9848\n",
            "Epoch 969/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0534 - accuracy: 0.9829 - val_loss: 0.0573 - val_accuracy: 0.9848\n",
            "Epoch 970/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0496 - accuracy: 0.9806 - val_loss: 0.0530 - val_accuracy: 0.9863\n",
            "Epoch 971/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0529 - accuracy: 0.9806 - val_loss: 0.0754 - val_accuracy: 0.9741\n",
            "Epoch 972/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0517 - accuracy: 0.9836 - val_loss: 0.0607 - val_accuracy: 0.9832\n",
            "Epoch 973/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0477 - accuracy: 0.9840 - val_loss: 0.0807 - val_accuracy: 0.9710\n",
            "Epoch 974/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 0.9829 - val_loss: 0.0552 - val_accuracy: 0.9863\n",
            "Epoch 975/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0512 - accuracy: 0.9833 - val_loss: 0.0716 - val_accuracy: 0.9741\n",
            "Epoch 976/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0506 - accuracy: 0.9802 - val_loss: 0.0506 - val_accuracy: 0.9832\n",
            "Epoch 977/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0672 - accuracy: 0.9760 - val_loss: 0.0535 - val_accuracy: 0.9817\n",
            "Epoch 978/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0687 - accuracy: 0.9726 - val_loss: 0.0529 - val_accuracy: 0.9832\n",
            "Epoch 979/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0629 - accuracy: 0.9749 - val_loss: 0.0529 - val_accuracy: 0.9817\n",
            "Epoch 980/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0615 - accuracy: 0.9764 - val_loss: 0.0522 - val_accuracy: 0.9832\n",
            "Epoch 981/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0634 - accuracy: 0.9768 - val_loss: 0.0601 - val_accuracy: 0.9832\n",
            "Epoch 982/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0542 - accuracy: 0.9791 - val_loss: 0.0487 - val_accuracy: 0.9863\n",
            "Epoch 983/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9810 - val_loss: 0.0583 - val_accuracy: 0.9802\n",
            "Epoch 984/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0610 - accuracy: 0.9775 - val_loss: 0.0659 - val_accuracy: 0.9817\n",
            "Epoch 985/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0616 - accuracy: 0.9775 - val_loss: 0.0507 - val_accuracy: 0.9878\n",
            "Epoch 986/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0599 - accuracy: 0.9775 - val_loss: 0.0538 - val_accuracy: 0.9832\n",
            "Epoch 987/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0636 - accuracy: 0.9787 - val_loss: 0.0507 - val_accuracy: 0.9848\n",
            "Epoch 988/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9798 - val_loss: 0.0473 - val_accuracy: 0.9848\n",
            "Epoch 989/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0613 - accuracy: 0.9783 - val_loss: 0.0570 - val_accuracy: 0.9863\n",
            "Epoch 990/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.9855 - val_loss: 0.0536 - val_accuracy: 0.9848\n",
            "Epoch 991/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0515 - accuracy: 0.9829 - val_loss: 0.0553 - val_accuracy: 0.9863\n",
            "Epoch 992/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0502 - accuracy: 0.9806 - val_loss: 0.0619 - val_accuracy: 0.9817\n",
            "Epoch 993/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0508 - accuracy: 0.9814 - val_loss: 0.0499 - val_accuracy: 0.9863\n",
            "Epoch 994/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0537 - accuracy: 0.9806 - val_loss: 0.0558 - val_accuracy: 0.9817\n",
            "Epoch 995/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0522 - accuracy: 0.9810 - val_loss: 0.0507 - val_accuracy: 0.9863\n",
            "Epoch 996/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0494 - accuracy: 0.9833 - val_loss: 0.0489 - val_accuracy: 0.9863\n",
            "Epoch 997/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0503 - accuracy: 0.9829 - val_loss: 0.0930 - val_accuracy: 0.9680\n",
            "Epoch 998/1000\n",
            "83/83 [==============================] - 0s 6ms/step - loss: 0.0650 - accuracy: 0.9768 - val_loss: 0.0779 - val_accuracy: 0.9726\n",
            "Epoch 999/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0535 - accuracy: 0.9817 - val_loss: 0.0525 - val_accuracy: 0.9832\n",
            "Epoch 1000/1000\n",
            "83/83 [==============================] - 1s 6ms/step - loss: 0.0533 - accuracy: 0.9836 - val_loss: 0.0600 - val_accuracy: 0.9832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(validation_data)\n",
        "predictions = [i.argmax() for i in preds]\n",
        "y_true = [i.argmax() for i in validation_labels]\n",
        "print('Accuracy {}'.format(accuracy_score(y_true=y_true, y_pred=predictions)))"
      ],
      "metadata": {
        "id": "sdHdp9c-p68K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6bbf60e-1f39-4b9d-db6e-28d72c03e580"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.9832317073170732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(test_data)\n",
        "\n",
        "predictions = [i.argmax() for i in preds]\n",
        "y_true = [i.argmax() for i in test_labels]\n",
        "#cm = confusion_matrix(y_pred=predictions, y_true=y_true)\n",
        "\n",
        "print('Accuracy {}'.format(accuracy_score(y_true=y_true, y_pred=predictions)))"
      ],
      "metadata": {
        "id": "KVVoY0z3Sn_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5de33b-4a07-47a6-cfaa-689a7f0889db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/vgg16_upto15frozen.h5')\n",
        "model.save_weights('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/vgg16_upto15frozen_weights.h5')"
      ],
      "metadata": {
        "id": "I_R7LeJrS1lC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model=load_model('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/vgg16_upto15frozen.h5',compile=False)\n",
        "loaded_model.load_weights('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/vgg16_upto15frozen_weights.h5')"
      ],
      "metadata": {
        "id": "KQGVlpsVTZrz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train','Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "pEXOQMeITu8-",
        "outputId": "9d92c190-3ef1-42b5-e89f-826a111791ce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train','Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ylwtfAxcT2iA",
        "outputId": "fe8e7dc6-4f2b-49b7-f2c9-c4e24da7a44f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels=train_generator_vgg16.classes\n",
        "train_labels=train_labels = to_categorical(train_labels, num_classes=2)\n",
        "validation_labels=val_generator_vgg16.classes\n",
        "validation_labels = to_categorical(validation_labels, num_classes=2)\n",
        "test_labels=test_generator_vgg16.classes\n",
        "test_labels=to_categorical(test_labels,num_classes=2)"
      ],
      "metadata": {
        "id": "HK3Gt04JT489"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data=np.load('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/bottleneck_features_validation_VGG16.npy')\n",
        "test_data=np.load('/content/drive/MyDrive/My_projects _and _datasets/IDRID_detection/bottleneck_features_test_VGG16.npy')"
      ],
      "metadata": {
        "id": "q62DNEpUUQvh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = loaded_model.predict(validation_data)\n",
        "predictions = [i.argmax() for i in preds]\n",
        "y_true = [i.argmax() for i in validation_labels]\n",
        "print('Accuracy {}'.format(accuracy_score(y_true=y_true, y_pred=predictions)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt3GKHbBUoea",
        "outputId": "65b3f436-3de2-48d2-f18b-128b8809253a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.9832317073170732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report \n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "p-wXWnG2Utkf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = loaded_model.predict(test_data)\n",
        "\n",
        "y_pred = [i.argmax() for i in preds]\n",
        "y_true = [i.argmax() for i in test_labels]\n",
        "cm = confusion_matrix(y_pred=y_pred, y_true=y_true)\n",
        "\n",
        "print('Test Accuracy={}'.format(accuracy_score(y_true=y_true, y_pred=y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xWSRYT6UyBx",
        "outputId": "27ca7daf-ed36-4a29-ebcc-86f7740bb546"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy=0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('CONFUSION MATRIX')\n",
        "conf_matrix = pd.DataFrame(data = cm,  \n",
        "                           columns = ['with_DR','without_DR'],  \n",
        "                           index =['with_DR','without_DR']) \n",
        "\n",
        "accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "misclass = 1 - accuracy\n",
        "plt.figure(figsize = (10,8)) \n",
        "sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = \"Blues\") \n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "plt.show() \n",
        "\n",
        "target_names=['with_DR','without_DR']\n",
        "print('The details for confusion matrix is =') \n",
        "print (classification_report(y_true, y_pred,target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "id": "VaMewLW1U1CL",
        "outputId": "963226f1-40d8-4984-a63f-e3f2b44fdb53"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFUSION MATRIX\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The details for confusion matrix is =\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     with_DR       0.99      0.97      0.98       181\n",
            "  without_DR       0.97      0.99      0.98       197\n",
            "\n",
            "    accuracy                           0.98       378\n",
            "   macro avg       0.98      0.98      0.98       378\n",
            "weighted avg       0.98      0.98      0.98       378\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sensitivity = cm[0,0]/(cm[0,0]+cm[1,0])\n",
        "print('Sensitivity : ', sensitivity*100 )\n",
        "\n",
        "Specificity = cm[1,1]/(cm[1,1]+cm[0,1])\n",
        "print('Specificity : ', Specificity*100 )"
      ],
      "metadata": {
        "id": "A4qCjFQPWYQS",
        "outputId": "742a9e6c-edcf-477a-d44b-44cfcf34bdbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  99.43181818181817\n",
            "Specificity :  97.02970297029702\n"
          ]
        }
      ]
    }
  ]
}